{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use. Here I'm creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the first 100 characters, make sure everything is peachy. According to the [American Book Review](http://americanbookreview.org/100bestlines.asp), this is the 6th best first line of a book ever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the characters encoded as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([48, 81, 22, 75,  0, 16, 29, 50, 64, 31, 31, 31, 15, 22, 75, 75,  3,\n",
       "       50, 44, 22, 67, 21, 17, 21, 16, 56, 50, 22, 29, 16, 50, 22, 17, 17,\n",
       "       50, 22, 17, 21, 80, 16, 41, 50, 16,  9, 16, 29,  3, 50, 28,  5, 81,\n",
       "       22, 75, 75,  3, 50, 44, 22, 67, 21, 17,  3, 50, 21, 56, 50, 28,  5,\n",
       "       81, 22, 75, 75,  3, 50, 21,  5, 50, 21,  0, 56, 50,  2, 49,  5, 31,\n",
       "       49, 22,  3, 42, 31, 31, 38,  9, 16, 29,  3,  0, 81, 21,  5], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the network is working with individual characters, it's similar to a classification problem in which we are trying to predict the next character from the previous text.  Here's how many 'classes' our network has to pick from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(chars)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making training and validation batches\n",
    "\n",
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the batch size. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the first split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll make my data sets and we can check out what's going on here. Here I'm going to use a batch size of 10 and 50 sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 178650)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the size of this array, we see that we have rows equal to the batch size. When we want to get a batch out of here, we can grab a subset of this array that contains all the rows but has a width equal to the number of steps in the sequence. The first batch looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[48, 81, 22, 75,  0, 16, 29, 50, 64, 31, 31, 31, 15, 22, 75, 75,  3,\n",
       "        50, 44, 22, 67, 21, 17, 21, 16, 56, 50, 22, 29, 16, 50, 22, 17, 17,\n",
       "        50, 22, 17, 21, 80, 16, 41, 50, 16,  9, 16, 29,  3, 50, 28,  5],\n",
       "       [50, 22, 67, 50,  5,  2,  0, 50, 30,  2, 21,  5, 30, 50,  0,  2, 50,\n",
       "        56,  0, 22,  3, 46, 79, 50, 22,  5, 56, 49, 16, 29, 16, 33, 50, 51,\n",
       "         5,  5, 22, 46, 50, 56, 67, 21, 17, 21,  5, 30, 46, 50, 72, 28],\n",
       "       [ 9, 21,  5, 42, 31, 31, 79, 34, 16, 56, 46, 50, 21,  0, 14, 56, 50,\n",
       "        56, 16,  0,  0, 17, 16, 33, 42, 50, 47, 81, 16, 50, 75, 29, 21, 66,\n",
       "        16, 50, 21, 56, 50, 67, 22, 30,  5, 21, 44, 21, 66, 16,  5,  0],\n",
       "       [ 5, 50, 33, 28, 29, 21,  5, 30, 50, 81, 21, 56, 50, 66,  2,  5,  9,\n",
       "        16, 29, 56, 22,  0, 21,  2,  5, 50, 49, 21,  0, 81, 50, 81, 21, 56,\n",
       "        31, 72, 29,  2,  0, 81, 16, 29, 50, 49, 22, 56, 50,  0, 81, 21],\n",
       "       [50, 21,  0, 50, 21, 56, 46, 50, 56, 21, 29, 40, 79, 50, 56, 22, 21,\n",
       "        33, 50,  0, 81, 16, 50,  2, 17, 33, 50, 67, 22,  5, 46, 50, 30, 16,\n",
       "         0,  0, 21,  5, 30, 50, 28, 75, 46, 50, 22,  5, 33, 31, 66, 29],\n",
       "       [50, 53,  0, 50, 49, 22, 56, 31,  2,  5, 17,  3, 50, 49, 81, 16,  5,\n",
       "        50,  0, 81, 16, 50, 56, 22, 67, 16, 50, 16,  9, 16,  5, 21,  5, 30,\n",
       "        50, 81, 16, 50, 66, 22, 67, 16, 50,  0,  2, 50,  0, 81, 16, 21],\n",
       "       [81, 16,  5, 50, 66,  2, 67, 16, 50, 44,  2, 29, 50, 67, 16, 46, 79,\n",
       "        50, 56, 81, 16, 50, 56, 22, 21, 33, 46, 50, 22,  5, 33, 50, 49, 16,\n",
       "         5,  0, 50, 72, 22, 66, 80, 50, 21,  5,  0,  2, 50,  0, 81, 16],\n",
       "       [41, 50, 72, 28,  0, 50,  5,  2, 49, 50, 56, 81, 16, 50, 49,  2, 28,\n",
       "        17, 33, 50, 29, 16, 22, 33, 21, 17,  3, 50, 81, 22,  9, 16, 50, 56,\n",
       "        22, 66, 29, 21, 44, 21, 66, 16, 33, 46, 50,  5,  2,  0, 50, 67],\n",
       "       [ 0, 50, 21, 56,  5, 14,  0, 42, 50, 47, 81, 16,  3, 14, 29, 16, 50,\n",
       "        75, 29,  2, 75, 29, 21, 16,  0,  2, 29, 56, 50,  2, 44, 50, 22, 50,\n",
       "        56,  2, 29,  0, 46, 31, 72, 28,  0, 50, 49, 16, 14, 29, 16, 50],\n",
       "       [50, 56, 22, 21, 33, 50,  0,  2, 50, 81, 16, 29, 56, 16, 17, 44, 46,\n",
       "        50, 22,  5, 33, 50, 72, 16, 30, 22,  5, 50, 22, 30, 22, 21,  5, 50,\n",
       "        44, 29,  2, 67, 50,  0, 81, 16, 50, 72, 16, 30, 21,  5,  5, 21]], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll write another function to grab batches out of the arrays made by `split_data`. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "Below is a function where I build the graph for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "    \n",
    "    # When we're using this network for sampling later, we'll be passing in\n",
    "    # one character at a time, so providing an option for that\n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # One-hot encoding the input and target characters\n",
    "    x_one_hot = tf.one_hot(inputs, num_classes)\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "\n",
    "    ### Build the RNN layers\n",
    "    # Use a basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    ### Run the data through the RNN layers\n",
    "    # This makes a list where each element is on step in the sequence\n",
    "    rnn_inputs = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(x_one_hot, num_steps, 1)]\n",
    "    \n",
    "    # Run each sequence step through the RNN and collect the outputs\n",
    "    outputs, state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=initial_state)\n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one output row for each step for each batch\n",
    "    seq_output = tf.concat(outputs, axis=1)\n",
    "    output = tf.reshape(seq_output, [-1, lstm_size])\n",
    "    \n",
    "    # Now connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and batch\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    preds = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    # Reshape the targets to match the logits\n",
    "    y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    # Export the nodes\n",
    "    # NOTE: I'm using a namedtuple here because I think they are cool\n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. \n",
    "\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `num_steps` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size` - The number of units in the hidden layers.\n",
    "* `num_layers` - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob` - The dropout keep probability when training. If you're network is overfitting, try decreasing this.\n",
    "\n",
    "Here's some good advice from Andrej Karpathy on training the network. I'm going to write it in here for your benefit, but also link to [where it originally came from](https://github.com/karpathy/char-rnn#tips-and-tricks).\n",
    "\n",
    "> ## Tips and Tricks\n",
    "\n",
    ">### Monitoring Validation Loss vs. Training Loss\n",
    ">If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "> - If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "> - If your training/validation loss are about equal then your model is **underfitting**. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "> ### Approximate number of parameters\n",
    "\n",
    "> The two most important parameters that control the model are `lstm_size` and `num_layers`. I would advise that you always use `num_layers` of either 2/3. The `lstm_size` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "> - The number of parameters in your model. This is printed when you start training.\n",
    "> - The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    ">These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "> - I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `lstm_size` larger.\n",
    "> - I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
    "\n",
    "> ### Best models strategy\n",
    "\n",
    ">The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    ">It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    ">By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100 \n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint.\n",
    "\n",
    "Here I'm saving checkpoints with the format\n",
    "\n",
    "`i{iteration number}_l{# hidden layer units}_v{validation loss}.ckpt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 1/3560 Training loss: 4.4192 7.3533 sec/batch\n",
      "Epoch 1/20  Iteration 2/3560 Training loss: 4.3711 6.2601 sec/batch\n",
      "Epoch 1/20  Iteration 3/3560 Training loss: 4.1779 6.3078 sec/batch\n",
      "Epoch 1/20  Iteration 4/3560 Training loss: 4.3685 6.5381 sec/batch\n",
      "Epoch 1/20  Iteration 5/3560 Training loss: 4.3393 6.8560 sec/batch\n",
      "Epoch 1/20  Iteration 6/3560 Training loss: 4.2673 6.4779 sec/batch\n",
      "Epoch 1/20  Iteration 7/3560 Training loss: 4.1946 6.5403 sec/batch\n",
      "Epoch 1/20  Iteration 8/3560 Training loss: 4.1238 6.4569 sec/batch\n",
      "Epoch 1/20  Iteration 9/3560 Training loss: 4.0564 6.4582 sec/batch\n",
      "Epoch 1/20  Iteration 10/3560 Training loss: 3.9986 6.5885 sec/batch\n",
      "Epoch 1/20  Iteration 11/3560 Training loss: 3.9475 6.7030 sec/batch\n",
      "Epoch 1/20  Iteration 12/3560 Training loss: 3.9025 6.7502 sec/batch\n",
      "Epoch 1/20  Iteration 13/3560 Training loss: 3.8632 6.5672 sec/batch\n",
      "Epoch 1/20  Iteration 14/3560 Training loss: 3.8303 6.5115 sec/batch\n",
      "Epoch 1/20  Iteration 15/3560 Training loss: 3.7998 6.4250 sec/batch\n",
      "Epoch 1/20  Iteration 16/3560 Training loss: 3.7715 7.2328 sec/batch\n",
      "Epoch 1/20  Iteration 17/3560 Training loss: 3.7446 6.5703 sec/batch\n",
      "Epoch 1/20  Iteration 18/3560 Training loss: 3.7224 7.4420 sec/batch\n",
      "Epoch 1/20  Iteration 19/3560 Training loss: 3.7004 6.4992 sec/batch\n",
      "Epoch 1/20  Iteration 20/3560 Training loss: 3.6787 7.3435 sec/batch\n",
      "Epoch 1/20  Iteration 21/3560 Training loss: 3.6597 7.6217 sec/batch\n",
      "Epoch 1/20  Iteration 22/3560 Training loss: 3.6419 7.8051 sec/batch\n",
      "Epoch 1/20  Iteration 23/3560 Training loss: 3.6250 8.1968 sec/batch\n",
      "Epoch 1/20  Iteration 24/3560 Training loss: 3.6101 8.3334 sec/batch\n",
      "Epoch 1/20  Iteration 25/3560 Training loss: 3.5953 6.7409 sec/batch\n",
      "Epoch 1/20  Iteration 26/3560 Training loss: 3.5824 6.3853 sec/batch\n",
      "Epoch 1/20  Iteration 27/3560 Training loss: 3.5704 6.4270 sec/batch\n",
      "Epoch 1/20  Iteration 28/3560 Training loss: 3.5583 6.4262 sec/batch\n",
      "Epoch 1/20  Iteration 29/3560 Training loss: 3.5469 6.5113 sec/batch\n",
      "Epoch 1/20  Iteration 30/3560 Training loss: 3.5361 6.4061 sec/batch\n",
      "Epoch 1/20  Iteration 31/3560 Training loss: 3.5268 6.4029 sec/batch\n",
      "Epoch 1/20  Iteration 32/3560 Training loss: 3.5170 6.4361 sec/batch\n",
      "Epoch 1/20  Iteration 33/3560 Training loss: 3.5077 6.4067 sec/batch\n",
      "Epoch 1/20  Iteration 34/3560 Training loss: 3.4991 6.3708 sec/batch\n",
      "Epoch 1/20  Iteration 35/3560 Training loss: 3.4903 6.3828 sec/batch\n",
      "Epoch 1/20  Iteration 36/3560 Training loss: 3.4825 6.3252 sec/batch\n",
      "Epoch 1/20  Iteration 37/3560 Training loss: 3.4745 6.3899 sec/batch\n",
      "Epoch 1/20  Iteration 38/3560 Training loss: 3.4667 6.3987 sec/batch\n",
      "Epoch 1/20  Iteration 39/3560 Training loss: 3.4593 6.3487 sec/batch\n",
      "Epoch 1/20  Iteration 40/3560 Training loss: 3.4523 6.4023 sec/batch\n",
      "Epoch 1/20  Iteration 41/3560 Training loss: 3.4454 6.5545 sec/batch\n",
      "Epoch 1/20  Iteration 42/3560 Training loss: 3.4388 6.3314 sec/batch\n",
      "Epoch 1/20  Iteration 43/3560 Training loss: 3.4325 6.4959 sec/batch\n",
      "Epoch 1/20  Iteration 44/3560 Training loss: 3.4264 6.4568 sec/batch\n",
      "Epoch 1/20  Iteration 45/3560 Training loss: 3.4203 6.4364 sec/batch\n",
      "Epoch 1/20  Iteration 46/3560 Training loss: 3.4149 6.5056 sec/batch\n",
      "Epoch 1/20  Iteration 47/3560 Training loss: 3.4097 6.4311 sec/batch\n",
      "Epoch 1/20  Iteration 48/3560 Training loss: 3.4049 6.4898 sec/batch\n",
      "Epoch 1/20  Iteration 49/3560 Training loss: 3.4002 6.5441 sec/batch\n",
      "Epoch 1/20  Iteration 50/3560 Training loss: 3.3957 6.5087 sec/batch\n",
      "Epoch 1/20  Iteration 51/3560 Training loss: 3.3911 6.5324 sec/batch\n",
      "Epoch 1/20  Iteration 52/3560 Training loss: 3.3866 6.5760 sec/batch\n",
      "Epoch 1/20  Iteration 53/3560 Training loss: 3.3824 6.5348 sec/batch\n",
      "Epoch 1/20  Iteration 54/3560 Training loss: 3.3779 6.6545 sec/batch\n",
      "Epoch 1/20  Iteration 55/3560 Training loss: 3.3740 6.5827 sec/batch\n",
      "Epoch 1/20  Iteration 56/3560 Training loss: 3.3697 6.5962 sec/batch\n",
      "Epoch 1/20  Iteration 57/3560 Training loss: 3.3658 6.5230 sec/batch\n",
      "Epoch 1/20  Iteration 58/3560 Training loss: 3.3621 6.7312 sec/batch\n",
      "Epoch 1/20  Iteration 59/3560 Training loss: 3.3583 6.6605 sec/batch\n",
      "Epoch 1/20  Iteration 60/3560 Training loss: 3.3548 6.6501 sec/batch\n",
      "Epoch 1/20  Iteration 61/3560 Training loss: 3.3514 6.6608 sec/batch\n",
      "Epoch 1/20  Iteration 62/3560 Training loss: 3.3483 6.6168 sec/batch\n",
      "Epoch 1/20  Iteration 63/3560 Training loss: 3.3454 6.6772 sec/batch\n",
      "Epoch 1/20  Iteration 64/3560 Training loss: 3.3420 6.6822 sec/batch\n",
      "Epoch 1/20  Iteration 65/3560 Training loss: 3.3386 7.1446 sec/batch\n",
      "Epoch 1/20  Iteration 66/3560 Training loss: 3.3358 7.7170 sec/batch\n",
      "Epoch 1/20  Iteration 67/3560 Training loss: 3.3330 6.7162 sec/batch\n",
      "Epoch 1/20  Iteration 68/3560 Training loss: 3.3295 6.6373 sec/batch\n",
      "Epoch 1/20  Iteration 69/3560 Training loss: 3.3266 6.7870 sec/batch\n",
      "Epoch 1/20  Iteration 70/3560 Training loss: 3.3239 6.7487 sec/batch\n",
      "Epoch 1/20  Iteration 71/3560 Training loss: 3.3211 6.5475 sec/batch\n",
      "Epoch 1/20  Iteration 72/3560 Training loss: 3.3186 6.7691 sec/batch\n",
      "Epoch 1/20  Iteration 73/3560 Training loss: 3.3159 6.6550 sec/batch\n",
      "Epoch 1/20  Iteration 74/3560 Training loss: 3.3133 6.6819 sec/batch\n",
      "Epoch 1/20  Iteration 75/3560 Training loss: 3.3109 6.7787 sec/batch\n",
      "Epoch 1/20  Iteration 76/3560 Training loss: 3.3085 6.6801 sec/batch\n",
      "Epoch 1/20  Iteration 77/3560 Training loss: 3.3061 6.7969 sec/batch\n",
      "Epoch 1/20  Iteration 78/3560 Training loss: 3.3036 6.7871 sec/batch\n",
      "Epoch 1/20  Iteration 79/3560 Training loss: 3.3012 6.7792 sec/batch\n",
      "Epoch 1/20  Iteration 80/3560 Training loss: 3.2986 6.7465 sec/batch\n",
      "Epoch 1/20  Iteration 81/3560 Training loss: 3.2963 6.7653 sec/batch\n",
      "Epoch 1/20  Iteration 82/3560 Training loss: 3.2943 6.7716 sec/batch\n",
      "Epoch 1/20  Iteration 83/3560 Training loss: 3.2922 6.7014 sec/batch\n",
      "Epoch 1/20  Iteration 84/3560 Training loss: 3.2900 6.7580 sec/batch\n",
      "Epoch 1/20  Iteration 85/3560 Training loss: 3.2877 6.7681 sec/batch\n",
      "Epoch 1/20  Iteration 86/3560 Training loss: 3.2855 6.6901 sec/batch\n",
      "Epoch 1/20  Iteration 87/3560 Training loss: 3.2832 6.7716 sec/batch\n",
      "Epoch 1/20  Iteration 88/3560 Training loss: 3.2811 6.7795 sec/batch\n",
      "Epoch 1/20  Iteration 89/3560 Training loss: 3.2791 6.7744 sec/batch\n",
      "Epoch 1/20  Iteration 90/3560 Training loss: 3.2771 6.7940 sec/batch\n",
      "Epoch 1/20  Iteration 91/3560 Training loss: 3.2751 6.8386 sec/batch\n",
      "Epoch 1/20  Iteration 92/3560 Training loss: 3.2731 6.7605 sec/batch\n",
      "Epoch 1/20  Iteration 93/3560 Training loss: 3.2709 6.8382 sec/batch\n",
      "Epoch 1/20  Iteration 94/3560 Training loss: 3.2689 6.8549 sec/batch\n",
      "Epoch 1/20  Iteration 95/3560 Training loss: 3.2667 6.9602 sec/batch\n",
      "Epoch 1/20  Iteration 96/3560 Training loss: 3.2645 6.7422 sec/batch\n",
      "Epoch 1/20  Iteration 97/3560 Training loss: 3.2626 6.9144 sec/batch\n",
      "Epoch 1/20  Iteration 98/3560 Training loss: 3.2615 6.7727 sec/batch\n",
      "Epoch 1/20  Iteration 99/3560 Training loss: 3.2596 6.8249 sec/batch\n",
      "Epoch 1/20  Iteration 100/3560 Training loss: 3.2577 6.8536 sec/batch\n",
      "Epoch 1/20  Iteration 101/3560 Training loss: 3.2560 6.9453 sec/batch\n",
      "Epoch 1/20  Iteration 102/3560 Training loss: 3.2542 6.7909 sec/batch\n",
      "Epoch 1/20  Iteration 103/3560 Training loss: 3.2523 6.9425 sec/batch\n",
      "Epoch 1/20  Iteration 104/3560 Training loss: 3.2504 6.8145 sec/batch\n",
      "Epoch 1/20  Iteration 105/3560 Training loss: 3.2486 6.8770 sec/batch\n",
      "Epoch 1/20  Iteration 106/3560 Training loss: 3.2468 6.8385 sec/batch\n",
      "Epoch 1/20  Iteration 107/3560 Training loss: 3.2448 6.9059 sec/batch\n",
      "Epoch 1/20  Iteration 108/3560 Training loss: 3.2428 6.8147 sec/batch\n",
      "Epoch 1/20  Iteration 109/3560 Training loss: 3.2410 6.9150 sec/batch\n",
      "Epoch 1/20  Iteration 110/3560 Training loss: 3.2388 6.9223 sec/batch\n",
      "Epoch 1/20  Iteration 111/3560 Training loss: 3.2368 6.8235 sec/batch\n",
      "Epoch 1/20  Iteration 112/3560 Training loss: 3.2349 7.0337 sec/batch\n",
      "Epoch 1/20  Iteration 113/3560 Training loss: 3.2328 6.8561 sec/batch\n",
      "Epoch 1/20  Iteration 114/3560 Training loss: 3.2306 6.8679 sec/batch\n",
      "Epoch 1/20  Iteration 115/3560 Training loss: 3.2284 6.8537 sec/batch\n",
      "Epoch 1/20  Iteration 116/3560 Training loss: 3.2262 6.9056 sec/batch\n",
      "Epoch 1/20  Iteration 117/3560 Training loss: 3.2241 6.8734 sec/batch\n",
      "Epoch 1/20  Iteration 118/3560 Training loss: 3.2221 6.8049 sec/batch\n",
      "Epoch 1/20  Iteration 119/3560 Training loss: 3.2200 6.9251 sec/batch\n",
      "Epoch 1/20  Iteration 120/3560 Training loss: 3.2178 6.9337 sec/batch\n",
      "Epoch 1/20  Iteration 121/3560 Training loss: 3.2157 6.8797 sec/batch\n",
      "Epoch 1/20  Iteration 122/3560 Training loss: 3.2134 6.9484 sec/batch\n",
      "Epoch 1/20  Iteration 123/3560 Training loss: 3.2112 6.9573 sec/batch\n",
      "Epoch 1/20  Iteration 124/3560 Training loss: 3.2089 6.8638 sec/batch\n",
      "Epoch 1/20  Iteration 125/3560 Training loss: 3.2064 6.9534 sec/batch\n",
      "Epoch 1/20  Iteration 126/3560 Training loss: 3.2037 6.9383 sec/batch\n",
      "Epoch 1/20  Iteration 127/3560 Training loss: 3.2012 6.8927 sec/batch\n",
      "Epoch 1/20  Iteration 128/3560 Training loss: 3.1988 6.9622 sec/batch\n",
      "Epoch 1/20  Iteration 129/3560 Training loss: 3.1970 7.0874 sec/batch\n",
      "Epoch 1/20  Iteration 130/3560 Training loss: 3.1959 6.9833 sec/batch\n",
      "Epoch 1/20  Iteration 131/3560 Training loss: 3.1947 6.9033 sec/batch\n",
      "Epoch 1/20  Iteration 132/3560 Training loss: 3.1930 6.9186 sec/batch\n",
      "Epoch 1/20  Iteration 133/3560 Training loss: 3.1911 6.9517 sec/batch\n",
      "Epoch 1/20  Iteration 134/3560 Training loss: 3.1891 6.9084 sec/batch\n",
      "Epoch 1/20  Iteration 135/3560 Training loss: 3.1871 6.9670 sec/batch\n",
      "Epoch 1/20  Iteration 136/3560 Training loss: 3.1851 6.9457 sec/batch\n",
      "Epoch 1/20  Iteration 137/3560 Training loss: 3.1831 6.9079 sec/batch\n",
      "Epoch 1/20  Iteration 138/3560 Training loss: 3.1810 6.8971 sec/batch\n",
      "Epoch 1/20  Iteration 139/3560 Training loss: 3.1791 6.9978 sec/batch\n",
      "Epoch 1/20  Iteration 140/3560 Training loss: 3.1771 6.9213 sec/batch\n",
      "Epoch 1/20  Iteration 141/3560 Training loss: 3.1752 6.9781 sec/batch\n",
      "Epoch 1/20  Iteration 142/3560 Training loss: 3.1730 6.9343 sec/batch\n",
      "Epoch 1/20  Iteration 143/3560 Training loss: 3.1708 6.9469 sec/batch\n",
      "Epoch 1/20  Iteration 144/3560 Training loss: 3.1686 6.9862 sec/batch\n",
      "Epoch 1/20  Iteration 145/3560 Training loss: 3.1664 7.1194 sec/batch\n",
      "Epoch 1/20  Iteration 146/3560 Training loss: 3.1643 7.1519 sec/batch\n",
      "Epoch 1/20  Iteration 147/3560 Training loss: 3.1620 7.0115 sec/batch\n",
      "Epoch 1/20  Iteration 148/3560 Training loss: 3.1598 6.9699 sec/batch\n",
      "Epoch 1/20  Iteration 149/3560 Training loss: 3.1574 7.0166 sec/batch\n",
      "Epoch 1/20  Iteration 150/3560 Training loss: 3.1551 6.9151 sec/batch\n",
      "Epoch 1/20  Iteration 151/3560 Training loss: 3.1529 6.9499 sec/batch\n",
      "Epoch 1/20  Iteration 152/3560 Training loss: 3.1508 6.9877 sec/batch\n",
      "Epoch 1/20  Iteration 153/3560 Training loss: 3.1485 6.9815 sec/batch\n",
      "Epoch 1/20  Iteration 154/3560 Training loss: 3.1461 6.9574 sec/batch\n",
      "Epoch 1/20  Iteration 155/3560 Training loss: 3.1436 7.0281 sec/batch\n",
      "Epoch 1/20  Iteration 156/3560 Training loss: 3.1410 6.9741 sec/batch\n",
      "Epoch 1/20  Iteration 157/3560 Training loss: 3.1384 6.9496 sec/batch\n",
      "Epoch 1/20  Iteration 158/3560 Training loss: 3.1357 7.0038 sec/batch\n",
      "Epoch 1/20  Iteration 159/3560 Training loss: 3.1330 7.0087 sec/batch\n",
      "Epoch 1/20  Iteration 160/3560 Training loss: 3.1304 6.9494 sec/batch\n",
      "Epoch 1/20  Iteration 161/3560 Training loss: 3.1278 7.0462 sec/batch\n",
      "Epoch 1/20  Iteration 162/3560 Training loss: 3.1250 7.0716 sec/batch\n",
      "Epoch 1/20  Iteration 163/3560 Training loss: 3.1222 6.9213 sec/batch\n",
      "Epoch 1/20  Iteration 164/3560 Training loss: 3.1195 7.3256 sec/batch\n",
      "Epoch 1/20  Iteration 165/3560 Training loss: 3.1169 7.2512 sec/batch\n",
      "Epoch 1/20  Iteration 166/3560 Training loss: 3.1141 7.0036 sec/batch\n",
      "Epoch 1/20  Iteration 167/3560 Training loss: 3.1114 6.9940 sec/batch\n",
      "Epoch 1/20  Iteration 168/3560 Training loss: 3.1087 6.9689 sec/batch\n",
      "Epoch 1/20  Iteration 169/3560 Training loss: 3.1060 7.0389 sec/batch\n",
      "Epoch 1/20  Iteration 170/3560 Training loss: 3.1031 6.9469 sec/batch\n",
      "Epoch 1/20  Iteration 171/3560 Training loss: 3.1004 7.1425 sec/batch\n",
      "Epoch 1/20  Iteration 172/3560 Training loss: 3.0978 6.9178 sec/batch\n",
      "Epoch 1/20  Iteration 173/3560 Training loss: 3.0953 7.0811 sec/batch\n",
      "Epoch 1/20  Iteration 174/3560 Training loss: 3.0927 7.0026 sec/batch\n",
      "Epoch 1/20  Iteration 175/3560 Training loss: 3.0903 7.1488 sec/batch\n",
      "Epoch 1/20  Iteration 176/3560 Training loss: 3.0884 6.9814 sec/batch\n",
      "Epoch 1/20  Iteration 177/3560 Training loss: 3.0862 7.0615 sec/batch\n",
      "Epoch 1/20  Iteration 178/3560 Training loss: 3.0833 7.0049 sec/batch\n",
      "Epoch 2/20  Iteration 179/3560 Training loss: 2.6838 7.1740 sec/batch\n",
      "Epoch 2/20  Iteration 180/3560 Training loss: 2.6320 6.9191 sec/batch\n",
      "Epoch 2/20  Iteration 181/3560 Training loss: 2.6096 7.2963 sec/batch\n",
      "Epoch 2/20  Iteration 182/3560 Training loss: 2.6057 7.0188 sec/batch\n",
      "Epoch 2/20  Iteration 183/3560 Training loss: 2.6013 7.0331 sec/batch\n",
      "Epoch 2/20  Iteration 184/3560 Training loss: 2.5955 7.0798 sec/batch\n",
      "Epoch 2/20  Iteration 185/3560 Training loss: 2.5907 6.9785 sec/batch\n",
      "Epoch 2/20  Iteration 186/3560 Training loss: 2.5896 7.0918 sec/batch\n",
      "Epoch 2/20  Iteration 187/3560 Training loss: 2.5881 6.9132 sec/batch\n",
      "Epoch 2/20  Iteration 188/3560 Training loss: 2.5839 7.0972 sec/batch\n",
      "Epoch 2/20  Iteration 189/3560 Training loss: 2.5790 6.9755 sec/batch\n",
      "Epoch 2/20  Iteration 190/3560 Training loss: 2.5763 7.0627 sec/batch\n",
      "Epoch 2/20  Iteration 191/3560 Training loss: 2.5734 7.0092 sec/batch\n",
      "Epoch 2/20  Iteration 192/3560 Training loss: 2.5727 7.1178 sec/batch\n",
      "Epoch 2/20  Iteration 193/3560 Training loss: 2.5700 6.9072 sec/batch\n",
      "Epoch 2/20  Iteration 194/3560 Training loss: 2.5678 7.1106 sec/batch\n",
      "Epoch 2/20  Iteration 195/3560 Training loss: 2.5662 7.0115 sec/batch\n",
      "Epoch 2/20  Iteration 196/3560 Training loss: 2.5661 7.0672 sec/batch\n",
      "Epoch 2/20  Iteration 197/3560 Training loss: 2.5637 7.1033 sec/batch\n",
      "Epoch 2/20  Iteration 198/3560 Training loss: 2.5599 7.1849 sec/batch\n",
      "Epoch 2/20  Iteration 199/3560 Training loss: 2.5568 7.0752 sec/batch\n",
      "Epoch 2/20  Iteration 200/3560 Training loss: 2.5559 268.9052 sec/batch\n",
      "Validation loss: 2.41792 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 201/3560 Training loss: 2.5540 6.6373 sec/batch\n",
      "Epoch 2/20  Iteration 202/3560 Training loss: 2.5515 6.6941 sec/batch\n",
      "Epoch 2/20  Iteration 203/3560 Training loss: 2.5487 6.7127 sec/batch\n",
      "Epoch 2/20  Iteration 204/3560 Training loss: 2.5465 6.7251 sec/batch\n",
      "Epoch 2/20  Iteration 205/3560 Training loss: 2.5441 6.7575 sec/batch\n",
      "Epoch 2/20  Iteration 206/3560 Training loss: 2.5418 6.7496 sec/batch\n",
      "Epoch 2/20  Iteration 207/3560 Training loss: 2.5400 6.5609 sec/batch\n",
      "Epoch 2/20  Iteration 208/3560 Training loss: 2.5380 6.8768 sec/batch\n",
      "Epoch 2/20  Iteration 209/3560 Training loss: 2.5365 6.9269 sec/batch\n",
      "Epoch 2/20  Iteration 210/3560 Training loss: 2.5337 7.1464 sec/batch\n",
      "Epoch 2/20  Iteration 211/3560 Training loss: 2.5311 7.1880 sec/batch\n",
      "Epoch 2/20  Iteration 212/3560 Training loss: 2.5293 7.2030 sec/batch\n",
      "Epoch 2/20  Iteration 213/3560 Training loss: 2.5269 6.9623 sec/batch\n",
      "Epoch 2/20  Iteration 214/3560 Training loss: 2.5252 6.9866 sec/batch\n",
      "Epoch 2/20  Iteration 215/3560 Training loss: 2.5230 6.9041 sec/batch\n",
      "Epoch 2/20  Iteration 216/3560 Training loss: 2.5203 6.8233 sec/batch\n",
      "Epoch 2/20  Iteration 217/3560 Training loss: 2.5179 6.7854 sec/batch\n",
      "Epoch 2/20  Iteration 218/3560 Training loss: 2.5157 6.7640 sec/batch\n",
      "Epoch 2/20  Iteration 219/3560 Training loss: 2.5133 6.6833 sec/batch\n",
      "Epoch 2/20  Iteration 220/3560 Training loss: 2.5111 6.7374 sec/batch\n",
      "Epoch 2/20  Iteration 221/3560 Training loss: 2.5090 6.8278 sec/batch\n",
      "Epoch 2/20  Iteration 222/3560 Training loss: 2.5067 6.7765 sec/batch\n",
      "Epoch 2/20  Iteration 223/3560 Training loss: 2.5046 6.7948 sec/batch\n",
      "Epoch 2/20  Iteration 224/3560 Training loss: 2.5018 7.0887 sec/batch\n",
      "Epoch 2/20  Iteration 225/3560 Training loss: 2.5002 6.7874 sec/batch\n",
      "Epoch 2/20  Iteration 226/3560 Training loss: 2.4984 6.9307 sec/batch\n",
      "Epoch 2/20  Iteration 227/3560 Training loss: 2.4964 6.8909 sec/batch\n",
      "Epoch 2/20  Iteration 228/3560 Training loss: 2.4950 6.9855 sec/batch\n",
      "Epoch 2/20  Iteration 229/3560 Training loss: 2.4930 6.7352 sec/batch\n",
      "Epoch 2/20  Iteration 230/3560 Training loss: 2.4914 6.8024 sec/batch\n",
      "Epoch 2/20  Iteration 231/3560 Training loss: 2.4895 6.8193 sec/batch\n",
      "Epoch 2/20  Iteration 232/3560 Training loss: 2.4876 6.7594 sec/batch\n",
      "Epoch 2/20  Iteration 233/3560 Training loss: 2.4858 6.8203 sec/batch\n",
      "Epoch 2/20  Iteration 234/3560 Training loss: 2.4842 6.8583 sec/batch\n",
      "Epoch 2/20  Iteration 235/3560 Training loss: 2.4826 6.7638 sec/batch\n",
      "Epoch 2/20  Iteration 236/3560 Training loss: 2.4808 6.8500 sec/batch\n",
      "Epoch 2/20  Iteration 237/3560 Training loss: 2.4790 6.7928 sec/batch\n",
      "Epoch 2/20  Iteration 238/3560 Training loss: 2.4778 6.7492 sec/batch\n",
      "Epoch 2/20  Iteration 239/3560 Training loss: 2.4761 6.7883 sec/batch\n",
      "Epoch 2/20  Iteration 240/3560 Training loss: 2.4749 6.8173 sec/batch\n",
      "Epoch 2/20  Iteration 241/3560 Training loss: 2.4736 6.9349 sec/batch\n",
      "Epoch 2/20  Iteration 242/3560 Training loss: 2.4720 6.8563 sec/batch\n",
      "Epoch 2/20  Iteration 243/3560 Training loss: 2.4703 6.9344 sec/batch\n",
      "Epoch 2/20  Iteration 244/3560 Training loss: 2.4690 6.8861 sec/batch\n",
      "Epoch 2/20  Iteration 245/3560 Training loss: 2.4673 6.8146 sec/batch\n",
      "Epoch 2/20  Iteration 246/3560 Training loss: 2.4655 6.9306 sec/batch\n",
      "Epoch 2/20  Iteration 247/3560 Training loss: 2.4636 6.8625 sec/batch\n",
      "Epoch 2/20  Iteration 248/3560 Training loss: 2.4622 6.9253 sec/batch\n",
      "Epoch 2/20  Iteration 249/3560 Training loss: 2.4609 6.8693 sec/batch\n",
      "Epoch 2/20  Iteration 250/3560 Training loss: 2.4596 6.8804 sec/batch\n",
      "Epoch 2/20  Iteration 251/3560 Training loss: 2.4581 6.8403 sec/batch\n",
      "Epoch 2/20  Iteration 252/3560 Training loss: 2.4566 6.9718 sec/batch\n",
      "Epoch 2/20  Iteration 253/3560 Training loss: 2.4551 6.9507 sec/batch\n",
      "Epoch 2/20  Iteration 254/3560 Training loss: 2.4541 6.8323 sec/batch\n",
      "Epoch 2/20  Iteration 255/3560 Training loss: 2.4527 7.1962 sec/batch\n",
      "Epoch 2/20  Iteration 256/3560 Training loss: 2.4514 6.9422 sec/batch\n",
      "Epoch 2/20  Iteration 257/3560 Training loss: 2.4497 6.9604 sec/batch\n",
      "Epoch 2/20  Iteration 258/3560 Training loss: 2.4482 6.9540 sec/batch\n",
      "Epoch 2/20  Iteration 259/3560 Training loss: 2.4467 7.1136 sec/batch\n",
      "Epoch 2/20  Iteration 260/3560 Training loss: 2.4454 6.9474 sec/batch\n",
      "Epoch 2/20  Iteration 261/3560 Training loss: 2.4438 6.9313 sec/batch\n",
      "Epoch 2/20  Iteration 262/3560 Training loss: 2.4422 6.9490 sec/batch\n",
      "Epoch 2/20  Iteration 263/3560 Training loss: 2.4404 6.9118 sec/batch\n",
      "Epoch 2/20  Iteration 264/3560 Training loss: 2.4389 6.8842 sec/batch\n",
      "Epoch 2/20  Iteration 265/3560 Training loss: 2.4375 6.9201 sec/batch\n",
      "Epoch 2/20  Iteration 266/3560 Training loss: 2.4360 6.9693 sec/batch\n",
      "Epoch 2/20  Iteration 267/3560 Training loss: 2.4344 6.8593 sec/batch\n",
      "Epoch 2/20  Iteration 268/3560 Training loss: 2.4332 6.8933 sec/batch\n",
      "Epoch 2/20  Iteration 269/3560 Training loss: 2.4318 6.9721 sec/batch\n",
      "Epoch 2/20  Iteration 270/3560 Training loss: 2.4305 6.8672 sec/batch\n",
      "Epoch 2/20  Iteration 271/3560 Training loss: 2.4289 6.9413 sec/batch\n",
      "Epoch 2/20  Iteration 272/3560 Training loss: 2.4274 6.9471 sec/batch\n",
      "Epoch 2/20  Iteration 273/3560 Training loss: 2.4259 6.9140 sec/batch\n",
      "Epoch 2/20  Iteration 274/3560 Training loss: 2.4244 7.1847 sec/batch\n",
      "Epoch 2/20  Iteration 275/3560 Training loss: 2.4230 7.2221 sec/batch\n",
      "Epoch 2/20  Iteration 276/3560 Training loss: 2.4216 7.1809 sec/batch\n",
      "Epoch 2/20  Iteration 277/3560 Training loss: 2.4200 6.9693 sec/batch\n",
      "Epoch 2/20  Iteration 278/3560 Training loss: 2.4186 7.0966 sec/batch\n",
      "Epoch 2/20  Iteration 279/3560 Training loss: 2.4175 7.0453 sec/batch\n",
      "Epoch 2/20  Iteration 280/3560 Training loss: 2.4162 6.9652 sec/batch\n",
      "Epoch 2/20  Iteration 281/3560 Training loss: 2.4147 7.0549 sec/batch\n",
      "Epoch 2/20  Iteration 282/3560 Training loss: 2.4133 7.0150 sec/batch\n",
      "Epoch 2/20  Iteration 283/3560 Training loss: 2.4118 6.9979 sec/batch\n",
      "Epoch 2/20  Iteration 284/3560 Training loss: 2.4106 7.1012 sec/batch\n",
      "Epoch 2/20  Iteration 285/3560 Training loss: 2.4093 6.9563 sec/batch\n",
      "Epoch 2/20  Iteration 286/3560 Training loss: 2.4083 8.5186 sec/batch\n",
      "Epoch 2/20  Iteration 287/3560 Training loss: 2.4071 7.2108 sec/batch\n",
      "Epoch 2/20  Iteration 288/3560 Training loss: 2.4057 6.9468 sec/batch\n",
      "Epoch 2/20  Iteration 289/3560 Training loss: 2.4046 7.1506 sec/batch\n",
      "Epoch 2/20  Iteration 290/3560 Training loss: 2.4035 6.9205 sec/batch\n",
      "Epoch 2/20  Iteration 291/3560 Training loss: 2.4022 7.0736 sec/batch\n",
      "Epoch 2/20  Iteration 292/3560 Training loss: 2.4008 6.9635 sec/batch\n",
      "Epoch 2/20  Iteration 293/3560 Training loss: 2.3995 7.2610 sec/batch\n",
      "Epoch 2/20  Iteration 294/3560 Training loss: 2.3980 6.9022 sec/batch\n",
      "Epoch 2/20  Iteration 295/3560 Training loss: 2.3967 7.1190 sec/batch\n",
      "Epoch 2/20  Iteration 296/3560 Training loss: 2.3955 6.9746 sec/batch\n",
      "Epoch 2/20  Iteration 297/3560 Training loss: 2.3944 7.0544 sec/batch\n",
      "Epoch 2/20  Iteration 298/3560 Training loss: 2.3932 6.9632 sec/batch\n",
      "Epoch 2/20  Iteration 299/3560 Training loss: 2.3921 7.1470 sec/batch\n",
      "Epoch 2/20  Iteration 300/3560 Training loss: 2.3908 6.9685 sec/batch\n",
      "Epoch 2/20  Iteration 301/3560 Training loss: 2.3895 7.1078 sec/batch\n",
      "Epoch 2/20  Iteration 302/3560 Training loss: 2.3884 6.9878 sec/batch\n",
      "Epoch 2/20  Iteration 303/3560 Training loss: 2.3871 7.1732 sec/batch\n",
      "Epoch 2/20  Iteration 304/3560 Training loss: 2.3857 6.9694 sec/batch\n",
      "Epoch 2/20  Iteration 305/3560 Training loss: 2.3846 7.1647 sec/batch\n",
      "Epoch 2/20  Iteration 306/3560 Training loss: 2.3836 6.9936 sec/batch\n",
      "Epoch 2/20  Iteration 307/3560 Training loss: 2.3824 7.1589 sec/batch\n",
      "Epoch 2/20  Iteration 308/3560 Training loss: 2.3812 7.0301 sec/batch\n",
      "Epoch 2/20  Iteration 309/3560 Training loss: 2.3799 7.1480 sec/batch\n",
      "Epoch 2/20  Iteration 310/3560 Training loss: 2.3785 7.1987 sec/batch\n",
      "Epoch 2/20  Iteration 311/3560 Training loss: 2.3774 7.0517 sec/batch\n",
      "Epoch 2/20  Iteration 312/3560 Training loss: 2.3764 7.0278 sec/batch\n",
      "Epoch 2/20  Iteration 313/3560 Training loss: 2.3751 7.1278 sec/batch\n",
      "Epoch 2/20  Iteration 314/3560 Training loss: 2.3740 6.9981 sec/batch\n",
      "Epoch 2/20  Iteration 315/3560 Training loss: 2.3729 7.1340 sec/batch\n",
      "Epoch 2/20  Iteration 316/3560 Training loss: 2.3718 7.1258 sec/batch\n",
      "Epoch 2/20  Iteration 317/3560 Training loss: 2.3708 6.9863 sec/batch\n",
      "Epoch 2/20  Iteration 318/3560 Training loss: 2.3696 7.0761 sec/batch\n",
      "Epoch 2/20  Iteration 319/3560 Training loss: 2.3686 7.1364 sec/batch\n",
      "Epoch 2/20  Iteration 320/3560 Training loss: 2.3675 7.1567 sec/batch\n",
      "Epoch 2/20  Iteration 321/3560 Training loss: 2.3664 7.2665 sec/batch\n",
      "Epoch 2/20  Iteration 322/3560 Training loss: 2.3653 6.9825 sec/batch\n",
      "Epoch 2/20  Iteration 323/3560 Training loss: 2.3641 7.0330 sec/batch\n",
      "Epoch 2/20  Iteration 324/3560 Training loss: 2.3632 7.0000 sec/batch\n",
      "Epoch 2/20  Iteration 325/3560 Training loss: 2.3622 7.0480 sec/batch\n",
      "Epoch 2/20  Iteration 326/3560 Training loss: 2.3613 7.0889 sec/batch\n",
      "Epoch 2/20  Iteration 327/3560 Training loss: 2.3602 6.9627 sec/batch\n",
      "Epoch 2/20  Iteration 328/3560 Training loss: 2.3591 6.9849 sec/batch\n",
      "Epoch 2/20  Iteration 329/3560 Training loss: 2.3580 7.0641 sec/batch\n",
      "Epoch 2/20  Iteration 330/3560 Training loss: 2.3572 7.0578 sec/batch\n",
      "Epoch 2/20  Iteration 331/3560 Training loss: 2.3562 7.0337 sec/batch\n",
      "Epoch 2/20  Iteration 332/3560 Training loss: 2.3552 1135.2077 sec/batch\n",
      "Epoch 2/20  Iteration 333/3560 Training loss: 2.3541 6.3350 sec/batch\n",
      "Epoch 2/20  Iteration 334/3560 Training loss: 2.3530 7.3911 sec/batch\n",
      "Epoch 2/20  Iteration 335/3560 Training loss: 2.3519 8.2548 sec/batch\n",
      "Epoch 2/20  Iteration 336/3560 Training loss: 2.3508 9.0233 sec/batch\n",
      "Epoch 2/20  Iteration 337/3560 Training loss: 2.3496 7.1697 sec/batch\n",
      "Epoch 2/20  Iteration 338/3560 Training loss: 2.3488 7.7809 sec/batch\n",
      "Epoch 2/20  Iteration 339/3560 Training loss: 2.3478 7.0923 sec/batch\n",
      "Epoch 2/20  Iteration 340/3560 Training loss: 2.3466 7.0495 sec/batch\n",
      "Epoch 2/20  Iteration 341/3560 Training loss: 2.3456 7.3341 sec/batch\n",
      "Epoch 2/20  Iteration 342/3560 Training loss: 2.3446 7.5134 sec/batch\n",
      "Epoch 2/20  Iteration 343/3560 Training loss: 2.3436 7.2334 sec/batch\n",
      "Epoch 2/20  Iteration 344/3560 Training loss: 2.3425 6.4548 sec/batch\n",
      "Epoch 2/20  Iteration 345/3560 Training loss: 2.3415 6.5305 sec/batch\n",
      "Epoch 2/20  Iteration 346/3560 Training loss: 2.3406 6.4114 sec/batch\n",
      "Epoch 2/20  Iteration 347/3560 Training loss: 2.3396 6.5025 sec/batch\n",
      "Epoch 2/20  Iteration 348/3560 Training loss: 2.3385 6.6316 sec/batch\n",
      "Epoch 2/20  Iteration 349/3560 Training loss: 2.3374 6.6902 sec/batch\n",
      "Epoch 2/20  Iteration 350/3560 Training loss: 2.3364 7.0943 sec/batch\n",
      "Epoch 2/20  Iteration 351/3560 Training loss: 2.3355 6.8075 sec/batch\n",
      "Epoch 2/20  Iteration 352/3560 Training loss: 2.3346 6.5423 sec/batch\n",
      "Epoch 2/20  Iteration 353/3560 Training loss: 2.3337 6.5529 sec/batch\n",
      "Epoch 2/20  Iteration 354/3560 Training loss: 2.3326 6.4541 sec/batch\n",
      "Epoch 2/20  Iteration 355/3560 Training loss: 2.3315 6.4476 sec/batch\n",
      "Epoch 2/20  Iteration 356/3560 Training loss: 2.3305 6.4932 sec/batch\n",
      "Epoch 3/20  Iteration 357/3560 Training loss: 2.2212 6.5404 sec/batch\n",
      "Epoch 3/20  Iteration 358/3560 Training loss: 2.1716 6.4469 sec/batch\n",
      "Epoch 3/20  Iteration 359/3560 Training loss: 2.1566 6.4878 sec/batch\n",
      "Epoch 3/20  Iteration 360/3560 Training loss: 2.1530 6.4307 sec/batch\n",
      "Epoch 3/20  Iteration 361/3560 Training loss: 2.1504 6.4375 sec/batch\n",
      "Epoch 3/20  Iteration 362/3560 Training loss: 2.1434 6.6223 sec/batch\n",
      "Epoch 3/20  Iteration 363/3560 Training loss: 2.1416 6.4018 sec/batch\n",
      "Epoch 3/20  Iteration 364/3560 Training loss: 2.1422 6.4331 sec/batch\n",
      "Epoch 3/20  Iteration 365/3560 Training loss: 2.1439 6.5161 sec/batch\n",
      "Epoch 3/20  Iteration 366/3560 Training loss: 2.1434 6.3293 sec/batch\n",
      "Epoch 3/20  Iteration 367/3560 Training loss: 2.1399 6.4682 sec/batch\n",
      "Epoch 3/20  Iteration 368/3560 Training loss: 2.1376 6.5040 sec/batch\n",
      "Epoch 3/20  Iteration 369/3560 Training loss: 2.1371 6.4091 sec/batch\n",
      "Epoch 3/20  Iteration 370/3560 Training loss: 2.1386 6.4582 sec/batch\n",
      "Epoch 3/20  Iteration 371/3560 Training loss: 2.1374 6.5037 sec/batch\n",
      "Epoch 3/20  Iteration 372/3560 Training loss: 2.1356 6.4279 sec/batch\n",
      "Epoch 3/20  Iteration 373/3560 Training loss: 2.1341 6.3882 sec/batch\n",
      "Epoch 3/20  Iteration 374/3560 Training loss: 2.1356 6.4583 sec/batch\n",
      "Epoch 3/20  Iteration 375/3560 Training loss: 2.1346 6.4417 sec/batch\n",
      "Epoch 3/20  Iteration 376/3560 Training loss: 2.1337 6.4142 sec/batch\n",
      "Epoch 3/20  Iteration 377/3560 Training loss: 2.1326 6.3151 sec/batch\n",
      "Epoch 3/20  Iteration 378/3560 Training loss: 2.1338 6.3473 sec/batch\n",
      "Epoch 3/20  Iteration 379/3560 Training loss: 2.1328 6.4037 sec/batch\n",
      "Epoch 3/20  Iteration 380/3560 Training loss: 2.1314 6.5446 sec/batch\n",
      "Epoch 3/20  Iteration 381/3560 Training loss: 2.1306 6.4060 sec/batch\n",
      "Epoch 3/20  Iteration 382/3560 Training loss: 2.1288 6.3508 sec/batch\n",
      "Epoch 3/20  Iteration 383/3560 Training loss: 2.1272 6.3110 sec/batch\n",
      "Epoch 3/20  Iteration 384/3560 Training loss: 2.1265 6.3967 sec/batch\n",
      "Epoch 3/20  Iteration 385/3560 Training loss: 2.1266 6.3312 sec/batch\n",
      "Epoch 3/20  Iteration 386/3560 Training loss: 2.1259 6.5035 sec/batch\n",
      "Epoch 3/20  Iteration 387/3560 Training loss: 2.1252 6.3629 sec/batch\n",
      "Epoch 3/20  Iteration 388/3560 Training loss: 2.1238 6.3205 sec/batch\n",
      "Epoch 3/20  Iteration 389/3560 Training loss: 2.1229 6.3924 sec/batch\n",
      "Epoch 3/20  Iteration 390/3560 Training loss: 2.1230 6.3644 sec/batch\n",
      "Epoch 3/20  Iteration 391/3560 Training loss: 2.1216 6.3691 sec/batch\n",
      "Epoch 3/20  Iteration 392/3560 Training loss: 2.1205 6.3428 sec/batch\n",
      "Epoch 3/20  Iteration 393/3560 Training loss: 2.1195 6.4135 sec/batch\n",
      "Epoch 3/20  Iteration 394/3560 Training loss: 2.1178 6.4418 sec/batch\n",
      "Epoch 3/20  Iteration 395/3560 Training loss: 2.1161 6.4218 sec/batch\n",
      "Epoch 3/20  Iteration 396/3560 Training loss: 2.1147 6.4491 sec/batch\n",
      "Epoch 3/20  Iteration 397/3560 Training loss: 2.1135 6.5492 sec/batch\n",
      "Epoch 3/20  Iteration 398/3560 Training loss: 2.1127 6.4926 sec/batch\n",
      "Epoch 3/20  Iteration 399/3560 Training loss: 2.1115 6.4985 sec/batch\n",
      "Epoch 3/20  Iteration 400/3560 Training loss: 2.1101 6.5335 sec/batch\n",
      "Validation loss: 1.9736 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 401/3560 Training loss: 2.1098 6.0360 sec/batch\n",
      "Epoch 3/20  Iteration 402/3560 Training loss: 2.1077 6.1868 sec/batch\n",
      "Epoch 3/20  Iteration 403/3560 Training loss: 2.1070 6.3064 sec/batch\n",
      "Epoch 3/20  Iteration 404/3560 Training loss: 2.1059 6.4268 sec/batch\n",
      "Epoch 3/20  Iteration 405/3560 Training loss: 2.1049 6.4455 sec/batch\n",
      "Epoch 3/20  Iteration 406/3560 Training loss: 2.1048 6.3668 sec/batch\n",
      "Epoch 3/20  Iteration 407/3560 Training loss: 2.1035 6.5506 sec/batch\n",
      "Epoch 3/20  Iteration 408/3560 Training loss: 2.1034 6.7390 sec/batch\n",
      "Epoch 3/20  Iteration 409/3560 Training loss: 2.1025 6.7583 sec/batch\n",
      "Epoch 3/20  Iteration 410/3560 Training loss: 2.1015 6.6485 sec/batch\n",
      "Epoch 3/20  Iteration 411/3560 Training loss: 2.1003 6.6851 sec/batch\n",
      "Epoch 3/20  Iteration 412/3560 Training loss: 2.0998 6.6757 sec/batch\n",
      "Epoch 3/20  Iteration 413/3560 Training loss: 2.0993 6.7327 sec/batch\n",
      "Epoch 3/20  Iteration 414/3560 Training loss: 2.0983 6.7527 sec/batch\n",
      "Epoch 3/20  Iteration 415/3560 Training loss: 2.0974 6.6650 sec/batch\n",
      "Epoch 3/20  Iteration 416/3560 Training loss: 2.0972 6.7054 sec/batch\n",
      "Epoch 3/20  Iteration 417/3560 Training loss: 2.0964 6.7791 sec/batch\n",
      "Epoch 3/20  Iteration 418/3560 Training loss: 2.0962 6.6242 sec/batch\n",
      "Epoch 3/20  Iteration 419/3560 Training loss: 2.0958 6.8666 sec/batch\n",
      "Epoch 3/20  Iteration 420/3560 Training loss: 2.0953 6.6642 sec/batch\n",
      "Epoch 3/20  Iteration 421/3560 Training loss: 2.0944 6.6674 sec/batch\n",
      "Epoch 3/20  Iteration 422/3560 Training loss: 2.0941 6.6980 sec/batch\n",
      "Epoch 3/20  Iteration 423/3560 Training loss: 2.0934 6.7150 sec/batch\n",
      "Epoch 3/20  Iteration 424/3560 Training loss: 2.0924 6.6299 sec/batch\n",
      "Epoch 3/20  Iteration 425/3560 Training loss: 2.0915 6.7187 sec/batch\n",
      "Epoch 3/20  Iteration 426/3560 Training loss: 2.0909 6.7857 sec/batch\n",
      "Epoch 3/20  Iteration 427/3560 Training loss: 2.0907 6.6902 sec/batch\n",
      "Epoch 3/20  Iteration 428/3560 Training loss: 2.0900 6.8128 sec/batch\n",
      "Epoch 3/20  Iteration 429/3560 Training loss: 2.0896 6.9078 sec/batch\n",
      "Epoch 3/20  Iteration 430/3560 Training loss: 2.0887 6.8533 sec/batch\n",
      "Epoch 3/20  Iteration 431/3560 Training loss: 2.0880 6.6676 sec/batch\n",
      "Epoch 3/20  Iteration 432/3560 Training loss: 2.0878 6.8753 sec/batch\n",
      "Epoch 3/20  Iteration 433/3560 Training loss: 2.0870 6.6435 sec/batch\n",
      "Epoch 3/20  Iteration 434/3560 Training loss: 2.0865 6.7962 sec/batch\n",
      "Epoch 3/20  Iteration 435/3560 Training loss: 2.0854 6.7701 sec/batch\n",
      "Epoch 3/20  Iteration 436/3560 Training loss: 2.0845 6.8785 sec/batch\n",
      "Epoch 3/20  Iteration 437/3560 Training loss: 2.0835 6.6600 sec/batch\n",
      "Epoch 3/20  Iteration 438/3560 Training loss: 2.0830 6.8472 sec/batch\n",
      "Epoch 3/20  Iteration 439/3560 Training loss: 2.0818 6.8653 sec/batch\n",
      "Epoch 3/20  Iteration 440/3560 Training loss: 2.0809 6.8183 sec/batch\n",
      "Epoch 3/20  Iteration 441/3560 Training loss: 2.0797 6.8432 sec/batch\n",
      "Epoch 3/20  Iteration 442/3560 Training loss: 2.0789 6.8206 sec/batch\n",
      "Epoch 3/20  Iteration 443/3560 Training loss: 2.0782 6.7849 sec/batch\n",
      "Epoch 3/20  Iteration 444/3560 Training loss: 2.0772 6.9089 sec/batch\n",
      "Epoch 3/20  Iteration 445/3560 Training loss: 2.0762 6.9405 sec/batch\n",
      "Epoch 3/20  Iteration 446/3560 Training loss: 2.0756 6.7959 sec/batch\n",
      "Epoch 3/20  Iteration 447/3560 Training loss: 2.0748 6.8510 sec/batch\n",
      "Epoch 3/20  Iteration 448/3560 Training loss: 2.0741 6.8336 sec/batch\n",
      "Epoch 3/20  Iteration 449/3560 Training loss: 2.0730 6.9099 sec/batch\n",
      "Epoch 3/20  Iteration 450/3560 Training loss: 2.0721 6.9112 sec/batch\n",
      "Epoch 3/20  Iteration 451/3560 Training loss: 2.0711 6.8259 sec/batch\n",
      "Epoch 3/20  Iteration 452/3560 Training loss: 2.0704 6.7823 sec/batch\n",
      "Epoch 3/20  Iteration 453/3560 Training loss: 2.0697 6.8515 sec/batch\n",
      "Epoch 3/20  Iteration 454/3560 Training loss: 2.0687 6.8936 sec/batch\n",
      "Epoch 3/20  Iteration 455/3560 Training loss: 2.0677 6.7771 sec/batch\n",
      "Epoch 3/20  Iteration 456/3560 Training loss: 2.0666 6.9588 sec/batch\n",
      "Epoch 3/20  Iteration 457/3560 Training loss: 2.0660 6.9550 sec/batch\n",
      "Epoch 3/20  Iteration 458/3560 Training loss: 2.0654 6.8289 sec/batch\n",
      "Epoch 3/20  Iteration 459/3560 Training loss: 2.0645 7.0341 sec/batch\n",
      "Epoch 3/20  Iteration 460/3560 Training loss: 2.0636 6.9311 sec/batch\n",
      "Epoch 3/20  Iteration 461/3560 Training loss: 2.0627 7.0034 sec/batch\n",
      "Epoch 3/20  Iteration 462/3560 Training loss: 2.0620 6.7981 sec/batch\n",
      "Epoch 3/20  Iteration 463/3560 Training loss: 2.0614 6.9397 sec/batch\n",
      "Epoch 3/20  Iteration 464/3560 Training loss: 2.0608 6.8929 sec/batch\n",
      "Epoch 3/20  Iteration 465/3560 Training loss: 2.0602 6.8560 sec/batch\n",
      "Epoch 3/20  Iteration 466/3560 Training loss: 2.0595 6.9080 sec/batch\n",
      "Epoch 3/20  Iteration 467/3560 Training loss: 2.0589 6.9595 sec/batch\n",
      "Epoch 3/20  Iteration 468/3560 Training loss: 2.0581 6.8578 sec/batch\n",
      "Epoch 3/20  Iteration 469/3560 Training loss: 2.0575 6.9930 sec/batch\n",
      "Epoch 3/20  Iteration 470/3560 Training loss: 2.0568 6.9555 sec/batch\n",
      "Epoch 3/20  Iteration 471/3560 Training loss: 2.0560 6.8233 sec/batch\n",
      "Epoch 3/20  Iteration 472/3560 Training loss: 2.0551 7.2238 sec/batch\n",
      "Epoch 3/20  Iteration 473/3560 Training loss: 2.0544 6.8721 sec/batch\n",
      "Epoch 3/20  Iteration 474/3560 Training loss: 2.0537 1374.5543 sec/batch\n",
      "Epoch 3/20  Iteration 475/3560 Training loss: 2.0530 6.4242 sec/batch\n",
      "Epoch 3/20  Iteration 476/3560 Training loss: 2.0525 6.8257 sec/batch\n",
      "Epoch 3/20  Iteration 477/3560 Training loss: 2.0518 8.2664 sec/batch\n",
      "Epoch 3/20  Iteration 478/3560 Training loss: 2.0509 8.9768 sec/batch\n",
      "Epoch 3/20  Iteration 479/3560 Training loss: 2.0501 7.0248 sec/batch\n",
      "Epoch 3/20  Iteration 480/3560 Training loss: 2.0497 7.1865 sec/batch\n",
      "Epoch 3/20  Iteration 481/3560 Training loss: 2.0490 6.8775 sec/batch\n",
      "Epoch 3/20  Iteration 482/3560 Training loss: 2.0481 6.6379 sec/batch\n",
      "Epoch 3/20  Iteration 483/3560 Training loss: 2.0476 7.1926 sec/batch\n",
      "Epoch 3/20  Iteration 484/3560 Training loss: 2.0470 6.7688 sec/batch\n",
      "Epoch 3/20  Iteration 485/3560 Training loss: 2.0465 7.0524 sec/batch\n",
      "Epoch 3/20  Iteration 486/3560 Training loss: 2.0459 6.8277 sec/batch\n",
      "Epoch 3/20  Iteration 487/3560 Training loss: 2.0450 7.1874 sec/batch\n",
      "Epoch 3/20  Iteration 488/3560 Training loss: 2.0442 7.1095 sec/batch\n",
      "Epoch 3/20  Iteration 489/3560 Training loss: 2.0435 6.9465 sec/batch\n",
      "Epoch 3/20  Iteration 490/3560 Training loss: 2.0430 9.6136 sec/batch\n",
      "Epoch 3/20  Iteration 491/3560 Training loss: 2.0424 7.4290 sec/batch\n",
      "Epoch 3/20  Iteration 492/3560 Training loss: 2.0419 6.7758 sec/batch\n",
      "Epoch 3/20  Iteration 493/3560 Training loss: 2.0414 6.5955 sec/batch\n",
      "Epoch 3/20  Iteration 494/3560 Training loss: 2.0408 6.8716 sec/batch\n",
      "Epoch 3/20  Iteration 495/3560 Training loss: 2.0405 6.6352 sec/batch\n",
      "Epoch 3/20  Iteration 496/3560 Training loss: 2.0399 6.5199 sec/batch\n",
      "Epoch 3/20  Iteration 497/3560 Training loss: 2.0395 6.7157 sec/batch\n",
      "Epoch 3/20  Iteration 498/3560 Training loss: 2.0389 6.5604 sec/batch\n",
      "Epoch 3/20  Iteration 499/3560 Training loss: 2.0382 6.3956 sec/batch\n",
      "Epoch 3/20  Iteration 500/3560 Training loss: 2.0376 6.4866 sec/batch\n",
      "Epoch 3/20  Iteration 501/3560 Training loss: 2.0369 6.5097 sec/batch\n",
      "Epoch 3/20  Iteration 502/3560 Training loss: 2.0364 6.4844 sec/batch\n",
      "Epoch 3/20  Iteration 503/3560 Training loss: 2.0359 6.6214 sec/batch\n",
      "Epoch 3/20  Iteration 504/3560 Training loss: 2.0355 6.4822 sec/batch\n",
      "Epoch 3/20  Iteration 505/3560 Training loss: 2.0349 6.4765 sec/batch\n",
      "Epoch 3/20  Iteration 506/3560 Training loss: 2.0343 6.5021 sec/batch\n",
      "Epoch 3/20  Iteration 507/3560 Training loss: 2.0336 6.4103 sec/batch\n",
      "Epoch 3/20  Iteration 508/3560 Training loss: 2.0333 6.3847 sec/batch\n",
      "Epoch 3/20  Iteration 509/3560 Training loss: 2.0328 6.6079 sec/batch\n",
      "Epoch 3/20  Iteration 510/3560 Training loss: 2.0323 6.5776 sec/batch\n",
      "Epoch 3/20  Iteration 511/3560 Training loss: 2.0317 6.4793 sec/batch\n",
      "Epoch 3/20  Iteration 512/3560 Training loss: 2.0311 6.4843 sec/batch\n",
      "Epoch 3/20  Iteration 513/3560 Training loss: 2.0306 6.5615 sec/batch\n",
      "Epoch 3/20  Iteration 514/3560 Training loss: 2.0300 6.4356 sec/batch\n",
      "Epoch 3/20  Iteration 515/3560 Training loss: 2.0292 6.4332 sec/batch\n",
      "Epoch 3/20  Iteration 516/3560 Training loss: 2.0288 6.4080 sec/batch\n",
      "Epoch 3/20  Iteration 517/3560 Training loss: 2.0284 6.4737 sec/batch\n",
      "Epoch 3/20  Iteration 518/3560 Training loss: 2.0279 6.3913 sec/batch\n",
      "Epoch 3/20  Iteration 519/3560 Training loss: 2.0274 6.5689 sec/batch\n",
      "Epoch 3/20  Iteration 520/3560 Training loss: 2.0270 6.6295 sec/batch\n",
      "Epoch 3/20  Iteration 521/3560 Training loss: 2.0264 6.4809 sec/batch\n",
      "Epoch 3/20  Iteration 522/3560 Training loss: 2.0258 6.4404 sec/batch\n",
      "Epoch 3/20  Iteration 523/3560 Training loss: 2.0253 6.3513 sec/batch\n",
      "Epoch 3/20  Iteration 524/3560 Training loss: 2.0250 6.4020 sec/batch\n",
      "Epoch 3/20  Iteration 525/3560 Training loss: 2.0244 6.4629 sec/batch\n",
      "Epoch 3/20  Iteration 526/3560 Training loss: 2.0238 6.3392 sec/batch\n",
      "Epoch 3/20  Iteration 527/3560 Training loss: 2.0231 6.3676 sec/batch\n",
      "Epoch 3/20  Iteration 528/3560 Training loss: 2.0225 6.3513 sec/batch\n",
      "Epoch 3/20  Iteration 529/3560 Training loss: 2.0220 6.2910 sec/batch\n",
      "Epoch 3/20  Iteration 530/3560 Training loss: 2.0215 6.3736 sec/batch\n",
      "Epoch 3/20  Iteration 531/3560 Training loss: 2.0210 6.5961 sec/batch\n",
      "Epoch 3/20  Iteration 532/3560 Training loss: 2.0204 6.4261 sec/batch\n",
      "Epoch 3/20  Iteration 533/3560 Training loss: 2.0197 6.4498 sec/batch\n",
      "Epoch 3/20  Iteration 534/3560 Training loss: 2.0192 6.3862 sec/batch\n",
      "Epoch 4/20  Iteration 535/3560 Training loss: 1.9986 6.2753 sec/batch\n",
      "Epoch 4/20  Iteration 536/3560 Training loss: 1.9490 6.3920 sec/batch\n",
      "Epoch 4/20  Iteration 537/3560 Training loss: 1.9345 6.3486 sec/batch\n",
      "Epoch 4/20  Iteration 538/3560 Training loss: 1.9278 6.4111 sec/batch\n",
      "Epoch 4/20  Iteration 539/3560 Training loss: 1.9240 6.4445 sec/batch\n",
      "Epoch 4/20  Iteration 540/3560 Training loss: 1.9143 6.2852 sec/batch\n",
      "Epoch 4/20  Iteration 541/3560 Training loss: 1.9138 6.4787 sec/batch\n",
      "Epoch 4/20  Iteration 542/3560 Training loss: 1.9122 6.4913 sec/batch\n",
      "Epoch 4/20  Iteration 543/3560 Training loss: 1.9158 6.4525 sec/batch\n",
      "Epoch 4/20  Iteration 544/3560 Training loss: 1.9154 6.3681 sec/batch\n",
      "Epoch 4/20  Iteration 545/3560 Training loss: 1.9119 6.4506 sec/batch\n",
      "Epoch 4/20  Iteration 546/3560 Training loss: 1.9098 6.3222 sec/batch\n",
      "Epoch 4/20  Iteration 547/3560 Training loss: 1.9100 6.3940 sec/batch\n",
      "Epoch 4/20  Iteration 548/3560 Training loss: 1.9123 6.3473 sec/batch\n",
      "Epoch 4/20  Iteration 549/3560 Training loss: 1.9112 6.3309 sec/batch\n",
      "Epoch 4/20  Iteration 550/3560 Training loss: 1.9090 6.5251 sec/batch\n",
      "Epoch 4/20  Iteration 551/3560 Training loss: 1.9088 6.3211 sec/batch\n",
      "Epoch 4/20  Iteration 552/3560 Training loss: 1.9102 6.4178 sec/batch\n",
      "Epoch 4/20  Iteration 553/3560 Training loss: 1.9100 6.6033 sec/batch\n",
      "Epoch 4/20  Iteration 554/3560 Training loss: 1.9098 6.4692 sec/batch\n",
      "Epoch 4/20  Iteration 555/3560 Training loss: 1.9089 6.4955 sec/batch\n",
      "Epoch 4/20  Iteration 556/3560 Training loss: 1.9094 6.3875 sec/batch\n",
      "Epoch 4/20  Iteration 557/3560 Training loss: 1.9083 6.4559 sec/batch\n",
      "Epoch 4/20  Iteration 558/3560 Training loss: 1.9073 6.4729 sec/batch\n",
      "Epoch 4/20  Iteration 559/3560 Training loss: 1.9064 6.5006 sec/batch\n",
      "Epoch 4/20  Iteration 560/3560 Training loss: 1.9047 6.4092 sec/batch\n",
      "Epoch 4/20  Iteration 561/3560 Training loss: 1.9034 6.5393 sec/batch\n",
      "Epoch 4/20  Iteration 562/3560 Training loss: 1.9029 6.5354 sec/batch\n",
      "Epoch 4/20  Iteration 563/3560 Training loss: 1.9035 6.5416 sec/batch\n",
      "Epoch 4/20  Iteration 564/3560 Training loss: 1.9033 6.7034 sec/batch\n",
      "Epoch 4/20  Iteration 565/3560 Training loss: 1.9028 6.5918 sec/batch\n",
      "Epoch 4/20  Iteration 566/3560 Training loss: 1.9014 6.5407 sec/batch\n",
      "Epoch 4/20  Iteration 567/3560 Training loss: 1.9009 6.5771 sec/batch\n",
      "Epoch 4/20  Iteration 568/3560 Training loss: 1.9012 6.4927 sec/batch\n",
      "Epoch 4/20  Iteration 569/3560 Training loss: 1.9003 6.6953 sec/batch\n",
      "Epoch 4/20  Iteration 570/3560 Training loss: 1.8994 6.5520 sec/batch\n",
      "Epoch 4/20  Iteration 571/3560 Training loss: 1.8986 6.5430 sec/batch\n",
      "Epoch 4/20  Iteration 572/3560 Training loss: 1.8970 6.5355 sec/batch\n",
      "Epoch 4/20  Iteration 573/3560 Training loss: 1.8954 6.5796 sec/batch\n",
      "Epoch 4/20  Iteration 574/3560 Training loss: 1.8942 6.5024 sec/batch\n",
      "Epoch 4/20  Iteration 575/3560 Training loss: 1.8933 6.7537 sec/batch\n",
      "Epoch 4/20  Iteration 576/3560 Training loss: 1.8930 6.6981 sec/batch\n",
      "Epoch 4/20  Iteration 577/3560 Training loss: 1.8921 6.5296 sec/batch\n",
      "Epoch 4/20  Iteration 578/3560 Training loss: 1.8906 6.6001 sec/batch\n",
      "Epoch 4/20  Iteration 579/3560 Training loss: 1.8902 6.5874 sec/batch\n",
      "Epoch 4/20  Iteration 580/3560 Training loss: 1.8885 6.6398 sec/batch\n",
      "Epoch 4/20  Iteration 581/3560 Training loss: 1.8880 6.5549 sec/batch\n",
      "Epoch 4/20  Iteration 582/3560 Training loss: 1.8870 6.6820 sec/batch\n",
      "Epoch 4/20  Iteration 583/3560 Training loss: 1.8865 6.5106 sec/batch\n",
      "Epoch 4/20  Iteration 584/3560 Training loss: 1.8869 6.7004 sec/batch\n",
      "Epoch 4/20  Iteration 585/3560 Training loss: 1.8857 6.5671 sec/batch\n",
      "Epoch 4/20  Iteration 586/3560 Training loss: 1.8863 6.7743 sec/batch\n",
      "Epoch 4/20  Iteration 587/3560 Training loss: 1.8855 6.9222 sec/batch\n",
      "Epoch 4/20  Iteration 588/3560 Training loss: 1.8849 6.6655 sec/batch\n",
      "Epoch 4/20  Iteration 589/3560 Training loss: 1.8842 6.5808 sec/batch\n",
      "Epoch 4/20  Iteration 590/3560 Training loss: 1.8839 6.6855 sec/batch\n",
      "Epoch 4/20  Iteration 591/3560 Training loss: 1.8836 6.6795 sec/batch\n",
      "Epoch 4/20  Iteration 592/3560 Training loss: 1.8829 6.6572 sec/batch\n",
      "Epoch 4/20  Iteration 593/3560 Training loss: 1.8821 6.7119 sec/batch\n",
      "Epoch 4/20  Iteration 594/3560 Training loss: 1.8822 6.6546 sec/batch\n",
      "Epoch 4/20  Iteration 595/3560 Training loss: 1.8816 6.6343 sec/batch\n",
      "Epoch 4/20  Iteration 596/3560 Training loss: 1.8818 6.6907 sec/batch\n",
      "Epoch 4/20  Iteration 597/3560 Training loss: 1.8819 6.8072 sec/batch\n",
      "Epoch 4/20  Iteration 598/3560 Training loss: 1.8818 6.7981 sec/batch\n",
      "Epoch 4/20  Iteration 599/3560 Training loss: 1.8814 6.7019 sec/batch\n",
      "Epoch 4/20  Iteration 600/3560 Training loss: 1.8814 6.7017 sec/batch\n",
      "Validation loss: 1.74782 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 601/3560 Training loss: 1.8817 6.1392 sec/batch\n",
      "Epoch 4/20  Iteration 602/3560 Training loss: 1.8810 6.3783 sec/batch\n",
      "Epoch 4/20  Iteration 603/3560 Training loss: 1.8806 6.5486 sec/batch\n",
      "Epoch 4/20  Iteration 604/3560 Training loss: 1.8802 6.6574 sec/batch\n",
      "Epoch 4/20  Iteration 605/3560 Training loss: 1.8803 6.5363 sec/batch\n",
      "Epoch 4/20  Iteration 606/3560 Training loss: 1.8801 6.6988 sec/batch\n",
      "Epoch 4/20  Iteration 607/3560 Training loss: 1.8801 6.7383 sec/batch\n",
      "Epoch 4/20  Iteration 608/3560 Training loss: 1.8794 6.8278 sec/batch\n",
      "Epoch 4/20  Iteration 609/3560 Training loss: 1.8789 6.9233 sec/batch\n",
      "Epoch 4/20  Iteration 610/3560 Training loss: 1.8787 6.8430 sec/batch\n",
      "Epoch 4/20  Iteration 611/3560 Training loss: 1.8783 6.8349 sec/batch\n",
      "Epoch 4/20  Iteration 612/3560 Training loss: 1.8780 6.7815 sec/batch\n",
      "Epoch 4/20  Iteration 613/3560 Training loss: 1.8771 6.7993 sec/batch\n",
      "Epoch 4/20  Iteration 614/3560 Training loss: 1.8765 6.9029 sec/batch\n",
      "Epoch 4/20  Iteration 615/3560 Training loss: 1.8757 6.7167 sec/batch\n",
      "Epoch 4/20  Iteration 616/3560 Training loss: 1.8754 6.8022 sec/batch\n",
      "Epoch 4/20  Iteration 617/3560 Training loss: 1.8745 6.7177 sec/batch\n",
      "Epoch 4/20  Iteration 618/3560 Training loss: 1.8740 6.7150 sec/batch\n",
      "Epoch 4/20  Iteration 619/3560 Training loss: 1.8733 6.8951 sec/batch\n",
      "Epoch 4/20  Iteration 620/3560 Training loss: 1.8726 7.2201 sec/batch\n",
      "Epoch 4/20  Iteration 621/3560 Training loss: 1.8721 6.7310 sec/batch\n",
      "Epoch 4/20  Iteration 622/3560 Training loss: 1.8715 6.7473 sec/batch\n",
      "Epoch 4/20  Iteration 623/3560 Training loss: 1.8706 6.7391 sec/batch\n",
      "Epoch 4/20  Iteration 624/3560 Training loss: 1.8704 6.6928 sec/batch\n",
      "Epoch 4/20  Iteration 625/3560 Training loss: 1.8698 6.7451 sec/batch\n",
      "Epoch 4/20  Iteration 626/3560 Training loss: 1.8694 6.7343 sec/batch\n",
      "Epoch 4/20  Iteration 627/3560 Training loss: 1.8686 6.7325 sec/batch\n",
      "Epoch 4/20  Iteration 628/3560 Training loss: 1.8680 6.6990 sec/batch\n",
      "Epoch 4/20  Iteration 629/3560 Training loss: 1.8674 6.7983 sec/batch\n",
      "Epoch 4/20  Iteration 630/3560 Training loss: 1.8670 6.8142 sec/batch\n",
      "Epoch 4/20  Iteration 631/3560 Training loss: 1.8665 145.4821 sec/batch\n",
      "Epoch 4/20  Iteration 632/3560 Training loss: 1.8658 7.0541 sec/batch\n",
      "Epoch 4/20  Iteration 633/3560 Training loss: 1.8652 6.6109 sec/batch\n",
      "Epoch 4/20  Iteration 634/3560 Training loss: 1.8643 8.0612 sec/batch\n",
      "Epoch 4/20  Iteration 635/3560 Training loss: 1.8640 9.9919 sec/batch\n",
      "Epoch 4/20  Iteration 636/3560 Training loss: 1.8636 7.9939 sec/batch\n",
      "Epoch 4/20  Iteration 637/3560 Training loss: 1.8630 7.2855 sec/batch\n",
      "Epoch 4/20  Iteration 638/3560 Training loss: 1.8625 6.7836 sec/batch\n",
      "Epoch 4/20  Iteration 639/3560 Training loss: 1.8619 7.6221 sec/batch\n",
      "Epoch 4/20  Iteration 640/3560 Training loss: 1.8615 8.1812 sec/batch\n",
      "Epoch 4/20  Iteration 641/3560 Training loss: 1.8610 6.8294 sec/batch\n",
      "Epoch 4/20  Iteration 642/3560 Training loss: 1.8607 6.6726 sec/batch\n",
      "Epoch 4/20  Iteration 643/3560 Training loss: 1.8604 6.7186 sec/batch\n",
      "Epoch 4/20  Iteration 644/3560 Training loss: 1.8600 6.6315 sec/batch\n",
      "Epoch 4/20  Iteration 645/3560 Training loss: 1.8595 6.8522 sec/batch\n",
      "Epoch 4/20  Iteration 646/3560 Training loss: 1.8590 7.4153 sec/batch\n",
      "Epoch 4/20  Iteration 647/3560 Training loss: 1.8585 7.5563 sec/batch\n",
      "Epoch 4/20  Iteration 648/3560 Training loss: 1.8580 7.3465 sec/batch\n",
      "Epoch 4/20  Iteration 649/3560 Training loss: 1.8573 7.1589 sec/batch\n",
      "Epoch 4/20  Iteration 650/3560 Training loss: 1.8566 6.9803 sec/batch\n",
      "Epoch 4/20  Iteration 651/3560 Training loss: 1.8562 6.9798 sec/batch\n",
      "Epoch 4/20  Iteration 652/3560 Training loss: 1.8557 6.7131 sec/batch\n",
      "Epoch 4/20  Iteration 653/3560 Training loss: 1.8552 6.6826 sec/batch\n",
      "Epoch 4/20  Iteration 654/3560 Training loss: 1.8548 6.6664 sec/batch\n",
      "Epoch 4/20  Iteration 655/3560 Training loss: 1.8545 6.5496 sec/batch\n",
      "Epoch 4/20  Iteration 656/3560 Training loss: 1.8537 6.6649 sec/batch\n",
      "Epoch 4/20  Iteration 657/3560 Training loss: 1.8530 6.6939 sec/batch\n",
      "Epoch 4/20  Iteration 658/3560 Training loss: 1.8527 6.7392 sec/batch\n",
      "Epoch 4/20  Iteration 659/3560 Training loss: 1.8523 6.7014 sec/batch\n",
      "Epoch 4/20  Iteration 660/3560 Training loss: 1.8515 6.8024 sec/batch\n",
      "Epoch 4/20  Iteration 661/3560 Training loss: 1.8513 6.7081 sec/batch\n",
      "Epoch 4/20  Iteration 662/3560 Training loss: 1.8510 6.8187 sec/batch\n",
      "Epoch 4/20  Iteration 663/3560 Training loss: 1.8506 6.8289 sec/batch\n",
      "Epoch 4/20  Iteration 664/3560 Training loss: 1.8501 6.7684 sec/batch\n",
      "Epoch 4/20  Iteration 665/3560 Training loss: 1.8494 6.8181 sec/batch\n",
      "Epoch 4/20  Iteration 666/3560 Training loss: 1.8487 6.8534 sec/batch\n",
      "Epoch 4/20  Iteration 667/3560 Training loss: 1.8483 6.7569 sec/batch\n",
      "Epoch 4/20  Iteration 668/3560 Training loss: 1.8479 6.7191 sec/batch\n",
      "Epoch 4/20  Iteration 669/3560 Training loss: 1.8474 6.7801 sec/batch\n",
      "Epoch 4/20  Iteration 670/3560 Training loss: 1.8471 6.6026 sec/batch\n",
      "Epoch 4/20  Iteration 671/3560 Training loss: 1.8467 6.6925 sec/batch\n",
      "Epoch 4/20  Iteration 672/3560 Training loss: 1.8464 6.8073 sec/batch\n",
      "Epoch 4/20  Iteration 673/3560 Training loss: 1.8461 7.4724 sec/batch\n",
      "Epoch 4/20  Iteration 674/3560 Training loss: 1.8457 6.6357 sec/batch\n",
      "Epoch 4/20  Iteration 675/3560 Training loss: 1.8455 6.6855 sec/batch\n",
      "Epoch 4/20  Iteration 676/3560 Training loss: 1.8450 6.6360 sec/batch\n",
      "Epoch 4/20  Iteration 677/3560 Training loss: 1.8446 6.5598 sec/batch\n",
      "Epoch 4/20  Iteration 678/3560 Training loss: 1.8442 6.6297 sec/batch\n",
      "Epoch 4/20  Iteration 679/3560 Training loss: 1.8437 6.6267 sec/batch\n",
      "Epoch 4/20  Iteration 680/3560 Training loss: 1.8434 6.6256 sec/batch\n",
      "Epoch 4/20  Iteration 681/3560 Training loss: 1.8430 6.6411 sec/batch\n",
      "Epoch 4/20  Iteration 682/3560 Training loss: 1.8428 6.6786 sec/batch\n",
      "Epoch 4/20  Iteration 683/3560 Training loss: 1.8425 6.6966 sec/batch\n",
      "Epoch 4/20  Iteration 684/3560 Training loss: 1.8421 6.6732 sec/batch\n",
      "Epoch 4/20  Iteration 685/3560 Training loss: 1.8415 6.6576 sec/batch\n",
      "Epoch 4/20  Iteration 686/3560 Training loss: 1.8413 6.6895 sec/batch\n",
      "Epoch 4/20  Iteration 687/3560 Training loss: 1.8409 6.7431 sec/batch\n",
      "Epoch 4/20  Iteration 688/3560 Training loss: 1.8405 6.6378 sec/batch\n",
      "Epoch 4/20  Iteration 689/3560 Training loss: 1.8402 6.6648 sec/batch\n",
      "Epoch 4/20  Iteration 690/3560 Training loss: 1.8398 6.6871 sec/batch\n",
      "Epoch 4/20  Iteration 691/3560 Training loss: 1.8396 6.5913 sec/batch\n",
      "Epoch 4/20  Iteration 692/3560 Training loss: 1.8392 6.5614 sec/batch\n",
      "Epoch 4/20  Iteration 693/3560 Training loss: 1.8387 6.7845 sec/batch\n",
      "Epoch 4/20  Iteration 694/3560 Training loss: 1.8385 6.7011 sec/batch\n",
      "Epoch 4/20  Iteration 695/3560 Training loss: 1.8383 6.7011 sec/batch\n",
      "Epoch 4/20  Iteration 696/3560 Training loss: 1.8379 6.6396 sec/batch\n",
      "Epoch 4/20  Iteration 697/3560 Training loss: 1.8377 6.8180 sec/batch\n",
      "Epoch 4/20  Iteration 698/3560 Training loss: 1.8374 6.6218 sec/batch\n",
      "Epoch 4/20  Iteration 699/3560 Training loss: 1.8370 6.7676 sec/batch\n",
      "Epoch 4/20  Iteration 700/3560 Training loss: 1.8366 6.5984 sec/batch\n",
      "Epoch 4/20  Iteration 701/3560 Training loss: 1.8364 6.7339 sec/batch\n",
      "Epoch 4/20  Iteration 702/3560 Training loss: 1.8365 6.7024 sec/batch\n",
      "Epoch 4/20  Iteration 703/3560 Training loss: 1.8361 6.8017 sec/batch\n",
      "Epoch 4/20  Iteration 704/3560 Training loss: 1.8358 6.6609 sec/batch\n",
      "Epoch 4/20  Iteration 705/3560 Training loss: 1.8353 6.7487 sec/batch\n",
      "Epoch 4/20  Iteration 706/3560 Training loss: 1.8348 6.6403 sec/batch\n",
      "Epoch 4/20  Iteration 707/3560 Training loss: 1.8345 6.7250 sec/batch\n",
      "Epoch 4/20  Iteration 708/3560 Training loss: 1.8342 6.7419 sec/batch\n",
      "Epoch 4/20  Iteration 709/3560 Training loss: 1.8339 6.6132 sec/batch\n",
      "Epoch 4/20  Iteration 710/3560 Training loss: 1.8335 6.7119 sec/batch\n",
      "Epoch 4/20  Iteration 711/3560 Training loss: 1.8330 6.8141 sec/batch\n",
      "Epoch 4/20  Iteration 712/3560 Training loss: 1.8327 6.6640 sec/batch\n",
      "Epoch 5/20  Iteration 713/3560 Training loss: 1.8492 6.7042 sec/batch\n",
      "Epoch 5/20  Iteration 714/3560 Training loss: 1.8022 6.7691 sec/batch\n",
      "Epoch 5/20  Iteration 715/3560 Training loss: 1.7868 6.7611 sec/batch\n",
      "Epoch 5/20  Iteration 716/3560 Training loss: 1.7791 6.7391 sec/batch\n",
      "Epoch 5/20  Iteration 717/3560 Training loss: 1.7752 6.7863 sec/batch\n",
      "Epoch 5/20  Iteration 718/3560 Training loss: 1.7637 6.7717 sec/batch\n",
      "Epoch 5/20  Iteration 719/3560 Training loss: 1.7651 6.7727 sec/batch\n",
      "Epoch 5/20  Iteration 720/3560 Training loss: 1.7637 6.8742 sec/batch\n",
      "Epoch 5/20  Iteration 721/3560 Training loss: 1.7660 6.7585 sec/batch\n",
      "Epoch 5/20  Iteration 722/3560 Training loss: 1.7661 6.7332 sec/batch\n",
      "Epoch 5/20  Iteration 723/3560 Training loss: 1.7628 6.8415 sec/batch\n",
      "Epoch 5/20  Iteration 724/3560 Training loss: 1.7608 6.7653 sec/batch\n",
      "Epoch 5/20  Iteration 725/3560 Training loss: 1.7598 6.6404 sec/batch\n",
      "Epoch 5/20  Iteration 726/3560 Training loss: 1.7621 6.7866 sec/batch\n",
      "Epoch 5/20  Iteration 727/3560 Training loss: 1.7611 6.7601 sec/batch\n",
      "Epoch 5/20  Iteration 728/3560 Training loss: 1.7596 6.7207 sec/batch\n",
      "Epoch 5/20  Iteration 729/3560 Training loss: 1.7602 6.8410 sec/batch\n",
      "Epoch 5/20  Iteration 730/3560 Training loss: 1.7620 6.7526 sec/batch\n",
      "Epoch 5/20  Iteration 731/3560 Training loss: 1.7616 6.6975 sec/batch\n",
      "Epoch 5/20  Iteration 732/3560 Training loss: 1.7631 6.8105 sec/batch\n",
      "Epoch 5/20  Iteration 733/3560 Training loss: 1.7621 7.4630 sec/batch\n",
      "Epoch 5/20  Iteration 734/3560 Training loss: 1.7630 9.2728 sec/batch\n",
      "Epoch 5/20  Iteration 735/3560 Training loss: 1.7618 6.9052 sec/batch\n",
      "Epoch 5/20  Iteration 736/3560 Training loss: 1.7617 6.9230 sec/batch\n",
      "Epoch 5/20  Iteration 737/3560 Training loss: 1.7613 6.7866 sec/batch\n",
      "Epoch 5/20  Iteration 738/3560 Training loss: 1.7591 6.7529 sec/batch\n",
      "Epoch 5/20  Iteration 739/3560 Training loss: 1.7578 6.7773 sec/batch\n",
      "Epoch 5/20  Iteration 740/3560 Training loss: 1.7579 6.7495 sec/batch\n",
      "Epoch 5/20  Iteration 741/3560 Training loss: 1.7584 6.7690 sec/batch\n",
      "Epoch 5/20  Iteration 742/3560 Training loss: 1.7581 6.7285 sec/batch\n",
      "Epoch 5/20  Iteration 743/3560 Training loss: 1.7576 6.7892 sec/batch\n",
      "Epoch 5/20  Iteration 744/3560 Training loss: 1.7566 6.7314 sec/batch\n",
      "Epoch 5/20  Iteration 745/3560 Training loss: 1.7563 6.7707 sec/batch\n",
      "Epoch 5/20  Iteration 746/3560 Training loss: 1.7567 6.7982 sec/batch\n",
      "Epoch 5/20  Iteration 747/3560 Training loss: 1.7561 6.7316 sec/batch\n",
      "Epoch 5/20  Iteration 748/3560 Training loss: 1.7556 6.7357 sec/batch\n",
      "Epoch 5/20  Iteration 749/3560 Training loss: 1.7547 6.7982 sec/batch\n",
      "Epoch 5/20  Iteration 750/3560 Training loss: 1.7532 6.7878 sec/batch\n",
      "Epoch 5/20  Iteration 751/3560 Training loss: 1.7515 6.8110 sec/batch\n",
      "Epoch 5/20  Iteration 752/3560 Training loss: 1.7504 6.7642 sec/batch\n",
      "Epoch 5/20  Iteration 753/3560 Training loss: 1.7496 6.7134 sec/batch\n",
      "Epoch 5/20  Iteration 754/3560 Training loss: 1.7492 6.9669 sec/batch\n",
      "Epoch 5/20  Iteration 755/3560 Training loss: 1.7484 6.7565 sec/batch\n",
      "Epoch 5/20  Iteration 756/3560 Training loss: 1.7472 7.0353 sec/batch\n",
      "Epoch 5/20  Iteration 757/3560 Training loss: 1.7471 6.7377 sec/batch\n",
      "Epoch 5/20  Iteration 758/3560 Training loss: 1.7458 6.8325 sec/batch\n",
      "Epoch 5/20  Iteration 759/3560 Training loss: 1.7452 6.8085 sec/batch\n",
      "Epoch 5/20  Iteration 760/3560 Training loss: 1.7443 6.8978 sec/batch\n",
      "Epoch 5/20  Iteration 761/3560 Training loss: 1.7438 6.7612 sec/batch\n",
      "Epoch 5/20  Iteration 762/3560 Training loss: 1.7441 6.7763 sec/batch\n",
      "Epoch 5/20  Iteration 763/3560 Training loss: 1.7432 6.7676 sec/batch\n",
      "Epoch 5/20  Iteration 764/3560 Training loss: 1.7438 6.8280 sec/batch\n",
      "Epoch 5/20  Iteration 765/3560 Training loss: 1.7431 6.8400 sec/batch\n",
      "Epoch 5/20  Iteration 766/3560 Training loss: 1.7429 6.8277 sec/batch\n",
      "Epoch 5/20  Iteration 767/3560 Training loss: 1.7421 6.7711 sec/batch\n",
      "Epoch 5/20  Iteration 768/3560 Training loss: 1.7419 6.8411 sec/batch\n",
      "Epoch 5/20  Iteration 769/3560 Training loss: 1.7420 6.7834 sec/batch\n",
      "Epoch 5/20  Iteration 770/3560 Training loss: 1.7416 6.8523 sec/batch\n",
      "Epoch 5/20  Iteration 771/3560 Training loss: 1.7408 6.9687 sec/batch\n",
      "Epoch 5/20  Iteration 772/3560 Training loss: 1.7411 7.4732 sec/batch\n",
      "Epoch 5/20  Iteration 773/3560 Training loss: 1.7407 7.3104 sec/batch\n",
      "Epoch 5/20  Iteration 774/3560 Training loss: 1.7412 6.7338 sec/batch\n",
      "Epoch 5/20  Iteration 775/3560 Training loss: 1.7413 6.7318 sec/batch\n",
      "Epoch 5/20  Iteration 776/3560 Training loss: 1.7414 6.8216 sec/batch\n",
      "Epoch 5/20  Iteration 777/3560 Training loss: 1.7410 6.8890 sec/batch\n",
      "Epoch 5/20  Iteration 778/3560 Training loss: 1.7412 6.8492 sec/batch\n",
      "Epoch 5/20  Iteration 779/3560 Training loss: 1.7412 6.8693 sec/batch\n",
      "Epoch 5/20  Iteration 780/3560 Training loss: 1.7406 8.1876 sec/batch\n",
      "Epoch 5/20  Iteration 781/3560 Training loss: 1.7403 6.9428 sec/batch\n",
      "Epoch 5/20  Iteration 782/3560 Training loss: 1.7401 6.7330 sec/batch\n",
      "Epoch 5/20  Iteration 783/3560 Training loss: 1.7403 6.8737 sec/batch\n",
      "Epoch 5/20  Iteration 784/3560 Training loss: 1.7402 6.8465 sec/batch\n",
      "Epoch 5/20  Iteration 785/3560 Training loss: 1.7404 6.7926 sec/batch\n",
      "Epoch 5/20  Iteration 786/3560 Training loss: 1.7399 6.9089 sec/batch\n",
      "Epoch 5/20  Iteration 787/3560 Training loss: 1.7395 6.8491 sec/batch\n",
      "Epoch 5/20  Iteration 788/3560 Training loss: 1.7395 6.8446 sec/batch\n",
      "Epoch 5/20  Iteration 789/3560 Training loss: 1.7391 6.9087 sec/batch\n",
      "Epoch 5/20  Iteration 790/3560 Training loss: 1.7390 6.8861 sec/batch\n",
      "Epoch 5/20  Iteration 791/3560 Training loss: 1.7382 6.7969 sec/batch\n",
      "Epoch 5/20  Iteration 792/3560 Training loss: 1.7378 6.8321 sec/batch\n",
      "Epoch 5/20  Iteration 793/3560 Training loss: 1.7372 6.8760 sec/batch\n",
      "Epoch 5/20  Iteration 794/3560 Training loss: 1.7371 6.8544 sec/batch\n",
      "Epoch 5/20  Iteration 795/3560 Training loss: 1.7363 6.8111 sec/batch\n",
      "Epoch 5/20  Iteration 796/3560 Training loss: 1.7359 6.8643 sec/batch\n",
      "Epoch 5/20  Iteration 797/3560 Training loss: 1.7352 6.8606 sec/batch\n",
      "Epoch 5/20  Iteration 798/3560 Training loss: 1.7347 6.7639 sec/batch\n",
      "Epoch 5/20  Iteration 799/3560 Training loss: 1.7342 8.3933 sec/batch\n",
      "Epoch 5/20  Iteration 800/3560 Training loss: 1.7337 7.0270 sec/batch\n",
      "Validation loss: 1.58989 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 801/3560 Training loss: 1.7336 6.1202 sec/batch\n",
      "Epoch 5/20  Iteration 802/3560 Training loss: 1.7334 6.4624 sec/batch\n",
      "Epoch 5/20  Iteration 803/3560 Training loss: 1.7328 6.6133 sec/batch\n",
      "Epoch 5/20  Iteration 804/3560 Training loss: 1.7324 6.6277 sec/batch\n",
      "Epoch 5/20  Iteration 805/3560 Training loss: 1.7317 6.7210 sec/batch\n",
      "Epoch 5/20  Iteration 806/3560 Training loss: 1.7311 6.7273 sec/batch\n",
      "Epoch 5/20  Iteration 807/3560 Training loss: 1.7304 6.6669 sec/batch\n",
      "Epoch 5/20  Iteration 808/3560 Training loss: 1.7300 6.9200 sec/batch\n",
      "Epoch 5/20  Iteration 809/3560 Training loss: 1.7297 6.9501 sec/batch\n",
      "Epoch 5/20  Iteration 810/3560 Training loss: 1.7290 6.8984 sec/batch\n",
      "Epoch 5/20  Iteration 811/3560 Training loss: 1.7284 6.9138 sec/batch\n",
      "Epoch 5/20  Iteration 812/3560 Training loss: 1.7276 6.9102 sec/batch\n",
      "Epoch 5/20  Iteration 813/3560 Training loss: 1.7272 6.9187 sec/batch\n",
      "Epoch 5/20  Iteration 814/3560 Training loss: 1.7268 6.8664 sec/batch\n",
      "Epoch 5/20  Iteration 815/3560 Training loss: 1.7263 6.9354 sec/batch\n",
      "Epoch 5/20  Iteration 816/3560 Training loss: 1.7258 6.8907 sec/batch\n",
      "Epoch 5/20  Iteration 817/3560 Training loss: 1.7253 6.7651 sec/batch\n",
      "Epoch 5/20  Iteration 818/3560 Training loss: 1.7250 6.8765 sec/batch\n",
      "Epoch 5/20  Iteration 819/3560 Training loss: 1.7246 6.8374 sec/batch\n",
      "Epoch 5/20  Iteration 820/3560 Training loss: 1.7242 6.7496 sec/batch\n",
      "Epoch 5/20  Iteration 821/3560 Training loss: 1.7239 6.9765 sec/batch\n",
      "Epoch 5/20  Iteration 822/3560 Training loss: 1.7236 6.8426 sec/batch\n",
      "Epoch 5/20  Iteration 823/3560 Training loss: 1.7232 6.7843 sec/batch\n",
      "Epoch 5/20  Iteration 824/3560 Training loss: 1.7227 6.8766 sec/batch\n",
      "Epoch 5/20  Iteration 825/3560 Training loss: 1.7223 6.8891 sec/batch\n",
      "Epoch 5/20  Iteration 826/3560 Training loss: 1.7219 6.7965 sec/batch\n",
      "Epoch 5/20  Iteration 827/3560 Training loss: 1.7214 6.8834 sec/batch\n",
      "Epoch 5/20  Iteration 828/3560 Training loss: 1.7207 6.8978 sec/batch\n",
      "Epoch 5/20  Iteration 829/3560 Training loss: 1.7203 6.7584 sec/batch\n",
      "Epoch 5/20  Iteration 830/3560 Training loss: 1.7199 6.9320 sec/batch\n",
      "Epoch 5/20  Iteration 831/3560 Training loss: 1.7194 6.8520 sec/batch\n",
      "Epoch 5/20  Iteration 832/3560 Training loss: 1.7191 9.7478 sec/batch\n",
      "Epoch 5/20  Iteration 833/3560 Training loss: 1.7188 7.9910 sec/batch\n",
      "Epoch 5/20  Iteration 834/3560 Training loss: 1.7182 6.9031 sec/batch\n",
      "Epoch 5/20  Iteration 835/3560 Training loss: 1.7176 6.8710 sec/batch\n",
      "Epoch 5/20  Iteration 836/3560 Training loss: 1.7173 6.9283 sec/batch\n",
      "Epoch 5/20  Iteration 837/3560 Training loss: 1.7170 6.8424 sec/batch\n",
      "Epoch 5/20  Iteration 838/3560 Training loss: 1.7164 6.8461 sec/batch\n",
      "Epoch 5/20  Iteration 839/3560 Training loss: 1.7163 6.9076 sec/batch\n",
      "Epoch 5/20  Iteration 840/3560 Training loss: 1.7161 6.8073 sec/batch\n",
      "Epoch 5/20  Iteration 841/3560 Training loss: 1.7157 6.8579 sec/batch\n",
      "Epoch 5/20  Iteration 842/3560 Training loss: 1.7153 6.8901 sec/batch\n",
      "Epoch 5/20  Iteration 843/3560 Training loss: 1.7146 6.7818 sec/batch\n",
      "Epoch 5/20  Iteration 844/3560 Training loss: 1.7142 6.8406 sec/batch\n",
      "Epoch 5/20  Iteration 845/3560 Training loss: 1.7140 6.8859 sec/batch\n",
      "Epoch 5/20  Iteration 846/3560 Training loss: 1.7138 6.7935 sec/batch\n",
      "Epoch 5/20  Iteration 847/3560 Training loss: 1.7136 6.9331 sec/batch\n",
      "Epoch 5/20  Iteration 848/3560 Training loss: 1.7134 6.9570 sec/batch\n",
      "Epoch 5/20  Iteration 849/3560 Training loss: 1.7133 7.1777 sec/batch\n",
      "Epoch 5/20  Iteration 850/3560 Training loss: 1.7130 6.8064 sec/batch\n",
      "Epoch 5/20  Iteration 851/3560 Training loss: 1.7129 7.0058 sec/batch\n",
      "Epoch 5/20  Iteration 852/3560 Training loss: 1.7125 6.9136 sec/batch\n",
      "Epoch 5/20  Iteration 853/3560 Training loss: 1.7126 6.9352 sec/batch\n",
      "Epoch 5/20  Iteration 854/3560 Training loss: 1.7122 6.8674 sec/batch\n",
      "Epoch 5/20  Iteration 855/3560 Training loss: 1.7119 6.9704 sec/batch\n",
      "Epoch 5/20  Iteration 856/3560 Training loss: 1.7118 6.7643 sec/batch\n",
      "Epoch 5/20  Iteration 857/3560 Training loss: 1.7114 6.8951 sec/batch\n",
      "Epoch 5/20  Iteration 858/3560 Training loss: 1.7112 6.8412 sec/batch\n",
      "Epoch 5/20  Iteration 859/3560 Training loss: 1.7110 6.7854 sec/batch\n",
      "Epoch 5/20  Iteration 860/3560 Training loss: 1.7109 6.8521 sec/batch\n",
      "Epoch 5/20  Iteration 861/3560 Training loss: 1.7107 6.9057 sec/batch\n",
      "Epoch 5/20  Iteration 862/3560 Training loss: 1.7103 6.8035 sec/batch\n",
      "Epoch 5/20  Iteration 863/3560 Training loss: 1.7098 6.9088 sec/batch\n",
      "Epoch 5/20  Iteration 864/3560 Training loss: 1.7095 6.8543 sec/batch\n",
      "Epoch 5/20  Iteration 865/3560 Training loss: 1.7093 6.7149 sec/batch\n",
      "Epoch 5/20  Iteration 866/3560 Training loss: 1.7091 6.9398 sec/batch\n",
      "Epoch 5/20  Iteration 867/3560 Training loss: 1.7087 6.9380 sec/batch\n",
      "Epoch 5/20  Iteration 868/3560 Training loss: 1.7084 6.8979 sec/batch\n",
      "Epoch 5/20  Iteration 869/3560 Training loss: 1.7082 6.8663 sec/batch\n",
      "Epoch 5/20  Iteration 870/3560 Training loss: 1.7079 6.8846 sec/batch\n",
      "Epoch 5/20  Iteration 871/3560 Training loss: 1.7074 6.8838 sec/batch\n",
      "Epoch 5/20  Iteration 872/3560 Training loss: 1.7072 6.8252 sec/batch\n",
      "Epoch 5/20  Iteration 873/3560 Training loss: 1.7071 6.9747 sec/batch\n",
      "Epoch 5/20  Iteration 874/3560 Training loss: 1.7068 6.8599 sec/batch\n",
      "Epoch 5/20  Iteration 875/3560 Training loss: 1.7066 6.7735 sec/batch\n",
      "Epoch 5/20  Iteration 876/3560 Training loss: 1.7063 6.8928 sec/batch\n",
      "Epoch 5/20  Iteration 877/3560 Training loss: 1.7060 6.8829 sec/batch\n",
      "Epoch 5/20  Iteration 878/3560 Training loss: 1.7057 6.7990 sec/batch\n",
      "Epoch 5/20  Iteration 879/3560 Training loss: 1.7055 6.8486 sec/batch\n",
      "Epoch 5/20  Iteration 880/3560 Training loss: 1.7057 6.8919 sec/batch\n",
      "Epoch 5/20  Iteration 881/3560 Training loss: 1.7053 6.8080 sec/batch\n",
      "Epoch 5/20  Iteration 882/3560 Training loss: 1.7051 6.8609 sec/batch\n",
      "Epoch 5/20  Iteration 883/3560 Training loss: 1.7047 6.8728 sec/batch\n",
      "Epoch 5/20  Iteration 884/3560 Training loss: 1.7043 6.9412 sec/batch\n",
      "Epoch 5/20  Iteration 885/3560 Training loss: 1.7041 6.8864 sec/batch\n",
      "Epoch 5/20  Iteration 886/3560 Training loss: 1.7039 6.8669 sec/batch\n",
      "Epoch 5/20  Iteration 887/3560 Training loss: 1.7037 6.8087 sec/batch\n",
      "Epoch 5/20  Iteration 888/3560 Training loss: 1.7034 6.8751 sec/batch\n",
      "Epoch 5/20  Iteration 889/3560 Training loss: 1.7030 6.8151 sec/batch\n",
      "Epoch 5/20  Iteration 890/3560 Training loss: 1.7028 6.8659 sec/batch\n",
      "Epoch 6/20  Iteration 891/3560 Training loss: 1.7377 6.8479 sec/batch\n",
      "Epoch 6/20  Iteration 892/3560 Training loss: 1.6891 6.8645 sec/batch\n",
      "Epoch 6/20  Iteration 893/3560 Training loss: 1.6747 6.8212 sec/batch\n",
      "Epoch 6/20  Iteration 894/3560 Training loss: 1.6685 6.7878 sec/batch\n",
      "Epoch 6/20  Iteration 895/3560 Training loss: 1.6616 6.8473 sec/batch\n",
      "Epoch 6/20  Iteration 896/3560 Training loss: 1.6514 6.8641 sec/batch\n",
      "Epoch 6/20  Iteration 897/3560 Training loss: 1.6511 6.7984 sec/batch\n",
      "Epoch 6/20  Iteration 898/3560 Training loss: 1.6487 6.8932 sec/batch\n",
      "Epoch 6/20  Iteration 899/3560 Training loss: 1.6507 6.9567 sec/batch\n",
      "Epoch 6/20  Iteration 900/3560 Training loss: 1.6504 6.7754 sec/batch\n",
      "Epoch 6/20  Iteration 901/3560 Training loss: 1.6472 7.0125 sec/batch\n",
      "Epoch 6/20  Iteration 902/3560 Training loss: 1.6455 6.9434 sec/batch\n",
      "Epoch 6/20  Iteration 903/3560 Training loss: 1.6448 6.7639 sec/batch\n",
      "Epoch 6/20  Iteration 904/3560 Training loss: 1.6472 6.8957 sec/batch\n",
      "Epoch 6/20  Iteration 905/3560 Training loss: 1.6462 6.8445 sec/batch\n",
      "Epoch 6/20  Iteration 906/3560 Training loss: 1.6448 6.8023 sec/batch\n",
      "Epoch 6/20  Iteration 907/3560 Training loss: 1.6452 6.8990 sec/batch\n",
      "Epoch 6/20  Iteration 908/3560 Training loss: 1.6465 6.8976 sec/batch\n",
      "Epoch 6/20  Iteration 909/3560 Training loss: 1.6465 6.7694 sec/batch\n",
      "Epoch 6/20  Iteration 910/3560 Training loss: 1.6474 6.8503 sec/batch\n",
      "Epoch 6/20  Iteration 911/3560 Training loss: 1.6468 6.9070 sec/batch\n",
      "Epoch 6/20  Iteration 912/3560 Training loss: 1.6476 6.9561 sec/batch\n",
      "Epoch 6/20  Iteration 913/3560 Training loss: 1.6466 6.8379 sec/batch\n",
      "Epoch 6/20  Iteration 914/3560 Training loss: 1.6462 6.8639 sec/batch\n",
      "Epoch 6/20  Iteration 915/3560 Training loss: 1.6464 6.8532 sec/batch\n",
      "Epoch 6/20  Iteration 916/3560 Training loss: 1.6451 6.8373 sec/batch\n",
      "Epoch 6/20  Iteration 917/3560 Training loss: 1.6436 6.8837 sec/batch\n",
      "Epoch 6/20  Iteration 918/3560 Training loss: 1.6438 6.8615 sec/batch\n",
      "Epoch 6/20  Iteration 919/3560 Training loss: 1.6444 6.9069 sec/batch\n",
      "Epoch 6/20  Iteration 920/3560 Training loss: 1.6444 6.8894 sec/batch\n",
      "Epoch 6/20  Iteration 921/3560 Training loss: 1.6440 6.9582 sec/batch\n",
      "Epoch 6/20  Iteration 922/3560 Training loss: 1.6431 6.8539 sec/batch\n",
      "Epoch 6/20  Iteration 923/3560 Training loss: 1.6430 6.9351 sec/batch\n",
      "Epoch 6/20  Iteration 924/3560 Training loss: 1.6433 6.8641 sec/batch\n",
      "Epoch 6/20  Iteration 925/3560 Training loss: 1.6427 6.7900 sec/batch\n",
      "Epoch 6/20  Iteration 926/3560 Training loss: 1.6422 6.9004 sec/batch\n",
      "Epoch 6/20  Iteration 927/3560 Training loss: 1.6414 6.8938 sec/batch\n",
      "Epoch 6/20  Iteration 928/3560 Training loss: 1.6401 6.8044 sec/batch\n",
      "Epoch 6/20  Iteration 929/3560 Training loss: 1.6385 6.8772 sec/batch\n",
      "Epoch 6/20  Iteration 930/3560 Training loss: 1.6378 6.9158 sec/batch\n",
      "Epoch 6/20  Iteration 931/3560 Training loss: 1.6371 6.8744 sec/batch\n",
      "Epoch 6/20  Iteration 932/3560 Training loss: 1.6374 6.8458 sec/batch\n",
      "Epoch 6/20  Iteration 933/3560 Training loss: 1.6366 6.9462 sec/batch\n",
      "Epoch 6/20  Iteration 934/3560 Training loss: 1.6357 6.8766 sec/batch\n",
      "Epoch 6/20  Iteration 935/3560 Training loss: 1.6357 6.8588 sec/batch\n",
      "Epoch 6/20  Iteration 936/3560 Training loss: 1.6345 7.0684 sec/batch\n",
      "Epoch 6/20  Iteration 937/3560 Training loss: 1.6338 6.8571 sec/batch\n",
      "Epoch 6/20  Iteration 938/3560 Training loss: 1.6331 6.8320 sec/batch\n",
      "Epoch 6/20  Iteration 939/3560 Training loss: 1.6326 6.8453 sec/batch\n",
      "Epoch 6/20  Iteration 940/3560 Training loss: 1.6330 6.8461 sec/batch\n",
      "Epoch 6/20  Iteration 941/3560 Training loss: 1.6321 6.8290 sec/batch\n",
      "Epoch 6/20  Iteration 942/3560 Training loss: 1.6328 6.8932 sec/batch\n",
      "Epoch 6/20  Iteration 943/3560 Training loss: 1.6323 6.8770 sec/batch\n",
      "Epoch 6/20  Iteration 944/3560 Training loss: 1.6320 6.7707 sec/batch\n",
      "Epoch 6/20  Iteration 945/3560 Training loss: 1.6312 6.8993 sec/batch\n",
      "Epoch 6/20  Iteration 946/3560 Training loss: 1.6312 6.8974 sec/batch\n",
      "Epoch 6/20  Iteration 947/3560 Training loss: 1.6315 6.8376 sec/batch\n",
      "Epoch 6/20  Iteration 948/3560 Training loss: 1.6308 6.9068 sec/batch\n",
      "Epoch 6/20  Iteration 949/3560 Training loss: 1.6301 6.8526 sec/batch\n",
      "Epoch 6/20  Iteration 950/3560 Training loss: 1.6304 6.8373 sec/batch\n",
      "Epoch 6/20  Iteration 951/3560 Training loss: 1.6300 6.8392 sec/batch\n",
      "Epoch 6/20  Iteration 952/3560 Training loss: 1.6307 6.8694 sec/batch\n",
      "Epoch 6/20  Iteration 953/3560 Training loss: 1.6309 6.9127 sec/batch\n",
      "Epoch 6/20  Iteration 954/3560 Training loss: 1.6308 6.9513 sec/batch\n",
      "Epoch 6/20  Iteration 955/3560 Training loss: 1.6304 6.8941 sec/batch\n",
      "Epoch 6/20  Iteration 956/3560 Training loss: 1.6303 6.9153 sec/batch\n",
      "Epoch 6/20  Iteration 957/3560 Training loss: 1.6303 6.8454 sec/batch\n",
      "Epoch 6/20  Iteration 958/3560 Training loss: 1.6297 6.8836 sec/batch\n",
      "Epoch 6/20  Iteration 959/3560 Training loss: 1.6295 6.8718 sec/batch\n",
      "Epoch 6/20  Iteration 960/3560 Training loss: 1.6294 6.8208 sec/batch\n",
      "Epoch 6/20  Iteration 961/3560 Training loss: 1.6296 6.8427 sec/batch\n",
      "Epoch 6/20  Iteration 962/3560 Training loss: 1.6298 6.8774 sec/batch\n",
      "Epoch 6/20  Iteration 963/3560 Training loss: 1.6300 6.8036 sec/batch\n",
      "Epoch 6/20  Iteration 964/3560 Training loss: 1.6295 6.8669 sec/batch\n",
      "Epoch 6/20  Iteration 965/3560 Training loss: 1.6293 6.9365 sec/batch\n",
      "Epoch 6/20  Iteration 966/3560 Training loss: 1.6294 6.8041 sec/batch\n",
      "Epoch 6/20  Iteration 967/3560 Training loss: 1.6291 6.8478 sec/batch\n",
      "Epoch 6/20  Iteration 968/3560 Training loss: 1.6290 6.9572 sec/batch\n",
      "Epoch 6/20  Iteration 969/3560 Training loss: 1.6283 6.8240 sec/batch\n",
      "Epoch 6/20  Iteration 970/3560 Training loss: 1.6280 6.8675 sec/batch\n",
      "Epoch 6/20  Iteration 971/3560 Training loss: 1.6272 7.1015 sec/batch\n",
      "Epoch 6/20  Iteration 972/3560 Training loss: 1.6270 6.8756 sec/batch\n",
      "Epoch 6/20  Iteration 973/3560 Training loss: 1.6264 6.8158 sec/batch\n",
      "Epoch 6/20  Iteration 974/3560 Training loss: 1.6263 6.8998 sec/batch\n",
      "Epoch 6/20  Iteration 975/3560 Training loss: 1.6259 6.8684 sec/batch\n",
      "Epoch 6/20  Iteration 976/3560 Training loss: 1.6255 7.3260 sec/batch\n",
      "Epoch 6/20  Iteration 977/3560 Training loss: 1.6251 6.8428 sec/batch\n",
      "Epoch 6/20  Iteration 978/3560 Training loss: 1.6246 6.8391 sec/batch\n",
      "Epoch 6/20  Iteration 979/3560 Training loss: 1.6240 6.7910 sec/batch\n",
      "Epoch 6/20  Iteration 980/3560 Training loss: 1.6239 6.8823 sec/batch\n",
      "Epoch 6/20  Iteration 981/3560 Training loss: 1.6235 6.8608 sec/batch\n",
      "Epoch 6/20  Iteration 982/3560 Training loss: 1.6232 6.8048 sec/batch\n",
      "Epoch 6/20  Iteration 983/3560 Training loss: 1.6226 6.8491 sec/batch\n",
      "Epoch 6/20  Iteration 984/3560 Training loss: 1.6220 6.8919 sec/batch\n",
      "Epoch 6/20  Iteration 985/3560 Training loss: 1.6215 6.7852 sec/batch\n",
      "Epoch 6/20  Iteration 986/3560 Training loss: 1.6214 6.8610 sec/batch\n",
      "Epoch 6/20  Iteration 987/3560 Training loss: 1.6212 6.9356 sec/batch\n",
      "Epoch 6/20  Iteration 988/3560 Training loss: 1.6205 6.8140 sec/batch\n",
      "Epoch 6/20  Iteration 989/3560 Training loss: 1.6201 7.0961 sec/batch\n",
      "Epoch 6/20  Iteration 990/3560 Training loss: 1.6195 6.8625 sec/batch\n",
      "Epoch 6/20  Iteration 991/3560 Training loss: 1.6194 6.8766 sec/batch\n",
      "Epoch 6/20  Iteration 992/3560 Training loss: 1.6191 6.8997 sec/batch\n",
      "Epoch 6/20  Iteration 993/3560 Training loss: 1.6188 6.8443 sec/batch\n",
      "Epoch 6/20  Iteration 994/3560 Training loss: 1.6185 6.8909 sec/batch\n",
      "Epoch 6/20  Iteration 995/3560 Training loss: 1.6182 6.8449 sec/batch\n",
      "Epoch 6/20  Iteration 996/3560 Training loss: 1.6180 6.9269 sec/batch\n",
      "Epoch 6/20  Iteration 997/3560 Training loss: 1.6178 6.8930 sec/batch\n",
      "Epoch 6/20  Iteration 998/3560 Training loss: 1.6176 6.8414 sec/batch\n",
      "Epoch 6/20  Iteration 999/3560 Training loss: 1.6174 6.8933 sec/batch\n",
      "Epoch 6/20  Iteration 1000/3560 Training loss: 1.6172 6.8675 sec/batch\n",
      "Validation loss: 1.47866 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 1001/3560 Training loss: 1.6176 6.2470 sec/batch\n",
      "Epoch 6/20  Iteration 1002/3560 Training loss: 1.6173 6.4303 sec/batch\n",
      "Epoch 6/20  Iteration 1003/3560 Training loss: 1.6170 6.7497 sec/batch\n",
      "Epoch 6/20  Iteration 1004/3560 Training loss: 1.6168 6.8336 sec/batch\n",
      "Epoch 6/20  Iteration 1005/3560 Training loss: 1.6163 6.7801 sec/batch\n",
      "Epoch 6/20  Iteration 1006/3560 Training loss: 1.6157 6.8035 sec/batch\n",
      "Epoch 6/20  Iteration 1007/3560 Training loss: 1.6156 6.8995 sec/batch\n",
      "Epoch 6/20  Iteration 1008/3560 Training loss: 1.6153 6.8851 sec/batch\n",
      "Epoch 6/20  Iteration 1009/3560 Training loss: 1.6150 6.8614 sec/batch\n",
      "Epoch 6/20  Iteration 1010/3560 Training loss: 1.6147 6.9986 sec/batch\n",
      "Epoch 6/20  Iteration 1011/3560 Training loss: 1.6145 6.9528 sec/batch\n",
      "Epoch 6/20  Iteration 1012/3560 Training loss: 1.6139 6.8470 sec/batch\n",
      "Epoch 6/20  Iteration 1013/3560 Training loss: 1.6134 7.2741 sec/batch\n",
      "Epoch 6/20  Iteration 1014/3560 Training loss: 1.6133 7.1293 sec/batch\n",
      "Epoch 6/20  Iteration 1015/3560 Training loss: 1.6131 6.9168 sec/batch\n",
      "Epoch 6/20  Iteration 1016/3560 Training loss: 1.6126 6.9152 sec/batch\n",
      "Epoch 6/20  Iteration 1017/3560 Training loss: 1.6125 7.0233 sec/batch\n",
      "Epoch 6/20  Iteration 1018/3560 Training loss: 1.6124 6.8031 sec/batch\n",
      "Epoch 6/20  Iteration 1019/3560 Training loss: 1.6122 6.9852 sec/batch\n",
      "Epoch 6/20  Iteration 1020/3560 Training loss: 1.6117 6.8585 sec/batch\n",
      "Epoch 6/20  Iteration 1021/3560 Training loss: 1.6112 6.8340 sec/batch\n",
      "Epoch 6/20  Iteration 1022/3560 Training loss: 1.6108 6.8832 sec/batch\n",
      "Epoch 6/20  Iteration 1023/3560 Training loss: 1.6107 6.9136 sec/batch\n",
      "Epoch 6/20  Iteration 1024/3560 Training loss: 1.6106 6.8506 sec/batch\n",
      "Epoch 6/20  Iteration 1025/3560 Training loss: 1.6104 7.0011 sec/batch\n",
      "Epoch 6/20  Iteration 1026/3560 Training loss: 1.6103 6.8572 sec/batch\n",
      "Epoch 6/20  Iteration 1027/3560 Training loss: 1.6103 7.0451 sec/batch\n",
      "Epoch 6/20  Iteration 1028/3560 Training loss: 1.6101 6.8036 sec/batch\n",
      "Epoch 6/20  Iteration 1029/3560 Training loss: 1.6101 7.0101 sec/batch\n",
      "Epoch 6/20  Iteration 1030/3560 Training loss: 1.6099 6.8406 sec/batch\n",
      "Epoch 6/20  Iteration 1031/3560 Training loss: 1.6101 6.8898 sec/batch\n",
      "Epoch 6/20  Iteration 1032/3560 Training loss: 1.6099 7.0016 sec/batch\n",
      "Epoch 6/20  Iteration 1033/3560 Training loss: 1.6097 7.0116 sec/batch\n",
      "Epoch 6/20  Iteration 1034/3560 Training loss: 1.6096 6.8353 sec/batch\n",
      "Epoch 6/20  Iteration 1035/3560 Training loss: 1.6093 8.9084 sec/batch\n",
      "Epoch 6/20  Iteration 1036/3560 Training loss: 1.6093 9.3037 sec/batch\n",
      "Epoch 6/20  Iteration 1037/3560 Training loss: 1.6091 7.0141 sec/batch\n",
      "Epoch 6/20  Iteration 1038/3560 Training loss: 1.6092 6.9733 sec/batch\n",
      "Epoch 6/20  Iteration 1039/3560 Training loss: 1.6090 6.8821 sec/batch\n",
      "Epoch 6/20  Iteration 1040/3560 Training loss: 1.6086 6.9757 sec/batch\n",
      "Epoch 6/20  Iteration 1041/3560 Training loss: 1.6081 6.8483 sec/batch\n",
      "Epoch 6/20  Iteration 1042/3560 Training loss: 1.6080 6.9012 sec/batch\n",
      "Epoch 6/20  Iteration 1043/3560 Training loss: 1.6078 6.8148 sec/batch\n",
      "Epoch 6/20  Iteration 1044/3560 Training loss: 1.6077 6.9380 sec/batch\n",
      "Epoch 6/20  Iteration 1045/3560 Training loss: 1.6076 6.8733 sec/batch\n",
      "Epoch 6/20  Iteration 1046/3560 Training loss: 1.6073 6.9159 sec/batch\n",
      "Epoch 6/20  Iteration 1047/3560 Training loss: 1.6072 6.8825 sec/batch\n",
      "Epoch 6/20  Iteration 1048/3560 Training loss: 1.6070 7.0164 sec/batch\n",
      "Epoch 6/20  Iteration 1049/3560 Training loss: 1.6065 6.9415 sec/batch\n",
      "Epoch 6/20  Iteration 1050/3560 Training loss: 1.6065 6.9235 sec/batch\n",
      "Epoch 6/20  Iteration 1051/3560 Training loss: 1.6065 6.8252 sec/batch\n",
      "Epoch 6/20  Iteration 1052/3560 Training loss: 1.6063 6.9798 sec/batch\n",
      "Epoch 6/20  Iteration 1053/3560 Training loss: 1.6062 6.9102 sec/batch\n",
      "Epoch 6/20  Iteration 1054/3560 Training loss: 1.6060 6.8780 sec/batch\n",
      "Epoch 6/20  Iteration 1055/3560 Training loss: 1.6058 6.8421 sec/batch\n",
      "Epoch 6/20  Iteration 1056/3560 Training loss: 1.6056 6.8824 sec/batch\n",
      "Epoch 6/20  Iteration 1057/3560 Training loss: 1.6056 6.8987 sec/batch\n",
      "Epoch 6/20  Iteration 1058/3560 Training loss: 1.6059 6.8411 sec/batch\n",
      "Epoch 6/20  Iteration 1059/3560 Training loss: 1.6057 6.8595 sec/batch\n",
      "Epoch 6/20  Iteration 1060/3560 Training loss: 1.6055 6.8713 sec/batch\n",
      "Epoch 6/20  Iteration 1061/3560 Training loss: 1.6053 6.8791 sec/batch\n",
      "Epoch 6/20  Iteration 1062/3560 Training loss: 1.6049 6.9034 sec/batch\n",
      "Epoch 6/20  Iteration 1063/3560 Training loss: 1.6048 6.9321 sec/batch\n",
      "Epoch 6/20  Iteration 1064/3560 Training loss: 1.6046 6.8813 sec/batch\n",
      "Epoch 6/20  Iteration 1065/3560 Training loss: 1.6045 7.0608 sec/batch\n",
      "Epoch 6/20  Iteration 1066/3560 Training loss: 1.6042 6.9692 sec/batch\n",
      "Epoch 6/20  Iteration 1067/3560 Training loss: 1.6040 6.8484 sec/batch\n",
      "Epoch 6/20  Iteration 1068/3560 Training loss: 1.6039 6.8980 sec/batch\n",
      "Epoch 7/20  Iteration 1069/3560 Training loss: 1.6623 6.8825 sec/batch\n",
      "Epoch 7/20  Iteration 1070/3560 Training loss: 1.6148 6.8929 sec/batch\n",
      "Epoch 7/20  Iteration 1071/3560 Training loss: 1.5980 6.9036 sec/batch\n",
      "Epoch 7/20  Iteration 1072/3560 Training loss: 1.5911 6.8820 sec/batch\n",
      "Epoch 7/20  Iteration 1073/3560 Training loss: 1.5829 6.8652 sec/batch\n",
      "Epoch 7/20  Iteration 1074/3560 Training loss: 1.5728 6.7931 sec/batch\n",
      "Epoch 7/20  Iteration 1075/3560 Training loss: 1.5745 6.8803 sec/batch\n",
      "Epoch 7/20  Iteration 1076/3560 Training loss: 1.5710 6.8885 sec/batch\n",
      "Epoch 7/20  Iteration 1077/3560 Training loss: 1.5724 6.8376 sec/batch\n",
      "Epoch 7/20  Iteration 1078/3560 Training loss: 1.5714 6.9381 sec/batch\n",
      "Epoch 7/20  Iteration 1079/3560 Training loss: 1.5676 6.9381 sec/batch\n",
      "Epoch 7/20  Iteration 1080/3560 Training loss: 1.5663 6.8277 sec/batch\n",
      "Epoch 7/20  Iteration 1081/3560 Training loss: 1.5658 6.9401 sec/batch\n",
      "Epoch 7/20  Iteration 1082/3560 Training loss: 1.5680 6.8924 sec/batch\n",
      "Epoch 7/20  Iteration 1083/3560 Training loss: 1.5670 7.0208 sec/batch\n",
      "Epoch 7/20  Iteration 1084/3560 Training loss: 1.5652 6.8309 sec/batch\n",
      "Epoch 7/20  Iteration 1085/3560 Training loss: 1.5653 6.8766 sec/batch\n",
      "Epoch 7/20  Iteration 1086/3560 Training loss: 1.5668 6.7955 sec/batch\n",
      "Epoch 7/20  Iteration 1087/3560 Training loss: 1.5669 6.9588 sec/batch\n",
      "Epoch 7/20  Iteration 1088/3560 Training loss: 1.5679 6.9136 sec/batch\n",
      "Epoch 7/20  Iteration 1089/3560 Training loss: 1.5672 6.8522 sec/batch\n",
      "Epoch 7/20  Iteration 1090/3560 Training loss: 1.5677 6.8132 sec/batch\n",
      "Epoch 7/20  Iteration 1091/3560 Training loss: 1.5668 6.9476 sec/batch\n",
      "Epoch 7/20  Iteration 1092/3560 Training loss: 1.5665 6.9223 sec/batch\n",
      "Epoch 7/20  Iteration 1093/3560 Training loss: 1.5664 6.8020 sec/batch\n",
      "Epoch 7/20  Iteration 1094/3560 Training loss: 1.5644 6.9092 sec/batch\n",
      "Epoch 7/20  Iteration 1095/3560 Training loss: 1.5631 6.8673 sec/batch\n",
      "Epoch 7/20  Iteration 1096/3560 Training loss: 1.5634 6.8993 sec/batch\n",
      "Epoch 7/20  Iteration 1097/3560 Training loss: 1.5633 6.8617 sec/batch\n",
      "Epoch 7/20  Iteration 1098/3560 Training loss: 1.5630 6.8966 sec/batch\n",
      "Epoch 7/20  Iteration 1099/3560 Training loss: 1.5624 6.7891 sec/batch\n",
      "Epoch 7/20  Iteration 1100/3560 Training loss: 1.5616 6.9758 sec/batch\n",
      "Epoch 7/20  Iteration 1101/3560 Training loss: 1.5619 7.0430 sec/batch\n",
      "Epoch 7/20  Iteration 1102/3560 Training loss: 1.5619 6.8114 sec/batch\n",
      "Epoch 7/20  Iteration 1103/3560 Training loss: 1.5615 6.8532 sec/batch\n",
      "Epoch 7/20  Iteration 1104/3560 Training loss: 1.5610 6.8751 sec/batch\n",
      "Epoch 7/20  Iteration 1105/3560 Training loss: 1.5598 6.8136 sec/batch\n",
      "Epoch 7/20  Iteration 1106/3560 Training loss: 1.5586 6.8719 sec/batch\n",
      "Epoch 7/20  Iteration 1107/3560 Training loss: 1.5569 6.9127 sec/batch\n",
      "Epoch 7/20  Iteration 1108/3560 Training loss: 1.5560 6.8056 sec/batch\n",
      "Epoch 7/20  Iteration 1109/3560 Training loss: 1.5556 6.8296 sec/batch\n",
      "Epoch 7/20  Iteration 1110/3560 Training loss: 1.5559 6.8569 sec/batch\n",
      "Epoch 7/20  Iteration 1111/3560 Training loss: 1.5553 6.8864 sec/batch\n",
      "Epoch 7/20  Iteration 1112/3560 Training loss: 1.5542 6.8204 sec/batch\n",
      "Epoch 7/20  Iteration 1113/3560 Training loss: 1.5544 6.9278 sec/batch\n",
      "Epoch 7/20  Iteration 1114/3560 Training loss: 1.5535 6.9387 sec/batch\n",
      "Epoch 7/20  Iteration 1115/3560 Training loss: 1.5533 6.8053 sec/batch\n",
      "Epoch 7/20  Iteration 1116/3560 Training loss: 1.5528 6.8964 sec/batch\n",
      "Epoch 7/20  Iteration 1117/3560 Training loss: 1.5524 6.8797 sec/batch\n",
      "Epoch 7/20  Iteration 1118/3560 Training loss: 1.5529 7.0780 sec/batch\n",
      "Epoch 7/20  Iteration 1119/3560 Training loss: 1.5524 6.9067 sec/batch\n",
      "Epoch 7/20  Iteration 1120/3560 Training loss: 1.5529 6.8495 sec/batch\n",
      "Epoch 7/20  Iteration 1121/3560 Training loss: 1.5528 6.8063 sec/batch\n",
      "Epoch 7/20  Iteration 1122/3560 Training loss: 1.5528 6.9231 sec/batch\n",
      "Epoch 7/20  Iteration 1123/3560 Training loss: 1.5525 6.9057 sec/batch\n",
      "Epoch 7/20  Iteration 1124/3560 Training loss: 1.5524 6.8078 sec/batch\n",
      "Epoch 7/20  Iteration 1125/3560 Training loss: 1.5527 6.8697 sec/batch\n",
      "Epoch 7/20  Iteration 1126/3560 Training loss: 1.5522 6.9070 sec/batch\n",
      "Epoch 7/20  Iteration 1127/3560 Training loss: 1.5517 6.9358 sec/batch\n",
      "Epoch 7/20  Iteration 1128/3560 Training loss: 1.5521 6.7822 sec/batch\n",
      "Epoch 7/20  Iteration 1129/3560 Training loss: 1.5519 6.9067 sec/batch\n",
      "Epoch 7/20  Iteration 1130/3560 Training loss: 1.5527 6.8645 sec/batch\n",
      "Epoch 7/20  Iteration 1131/3560 Training loss: 1.5531 6.8158 sec/batch\n",
      "Epoch 7/20  Iteration 1132/3560 Training loss: 1.5531 6.8862 sec/batch\n",
      "Epoch 7/20  Iteration 1133/3560 Training loss: 1.5530 6.9184 sec/batch\n",
      "Epoch 7/20  Iteration 1134/3560 Training loss: 1.5530 6.8073 sec/batch\n",
      "Epoch 7/20  Iteration 1135/3560 Training loss: 1.5530 7.0491 sec/batch\n",
      "Epoch 7/20  Iteration 1136/3560 Training loss: 1.5527 6.9584 sec/batch\n",
      "Epoch 7/20  Iteration 1137/3560 Training loss: 1.5526 6.8137 sec/batch\n",
      "Epoch 7/20  Iteration 1138/3560 Training loss: 1.5523 6.8693 sec/batch\n",
      "Epoch 7/20  Iteration 1139/3560 Training loss: 1.5527 6.8561 sec/batch\n",
      "Epoch 7/20  Iteration 1140/3560 Training loss: 1.5528 6.9070 sec/batch\n",
      "Epoch 7/20  Iteration 1141/3560 Training loss: 1.5531 6.8968 sec/batch\n",
      "Epoch 7/20  Iteration 1142/3560 Training loss: 1.5526 6.8891 sec/batch\n",
      "Epoch 7/20  Iteration 1143/3560 Training loss: 1.5523 6.8194 sec/batch\n",
      "Epoch 7/20  Iteration 1144/3560 Training loss: 1.5523 6.9043 sec/batch\n",
      "Epoch 7/20  Iteration 1145/3560 Training loss: 1.5520 6.8848 sec/batch\n",
      "Epoch 7/20  Iteration 1146/3560 Training loss: 1.5517 6.8424 sec/batch\n",
      "Epoch 7/20  Iteration 1147/3560 Training loss: 1.5510 6.8507 sec/batch\n",
      "Epoch 7/20  Iteration 1148/3560 Training loss: 1.5507 6.8986 sec/batch\n",
      "Epoch 7/20  Iteration 1149/3560 Training loss: 1.5501 6.8911 sec/batch\n",
      "Epoch 7/20  Iteration 1150/3560 Training loss: 1.5500 6.8591 sec/batch\n",
      "Epoch 7/20  Iteration 1151/3560 Training loss: 1.5493 6.8919 sec/batch\n",
      "Epoch 7/20  Iteration 1152/3560 Training loss: 1.5491 6.8508 sec/batch\n",
      "Epoch 7/20  Iteration 1153/3560 Training loss: 1.5486 7.0116 sec/batch\n",
      "Epoch 7/20  Iteration 1154/3560 Training loss: 1.5483 6.8906 sec/batch\n",
      "Epoch 7/20  Iteration 1155/3560 Training loss: 1.5478 6.8841 sec/batch\n",
      "Epoch 7/20  Iteration 1156/3560 Training loss: 1.5474 6.8049 sec/batch\n",
      "Epoch 7/20  Iteration 1157/3560 Training loss: 1.5468 6.9081 sec/batch\n",
      "Epoch 7/20  Iteration 1158/3560 Training loss: 1.5467 6.9029 sec/batch\n",
      "Epoch 7/20  Iteration 1159/3560 Training loss: 1.5462 6.8440 sec/batch\n",
      "Epoch 7/20  Iteration 1160/3560 Training loss: 1.5458 6.9359 sec/batch\n",
      "Epoch 7/20  Iteration 1161/3560 Training loss: 1.5454 6.9108 sec/batch\n",
      "Epoch 7/20  Iteration 1162/3560 Training loss: 1.5449 6.8629 sec/batch\n",
      "Epoch 7/20  Iteration 1163/3560 Training loss: 1.5445 6.9201 sec/batch\n",
      "Epoch 7/20  Iteration 1164/3560 Training loss: 1.5444 6.8658 sec/batch\n",
      "Epoch 7/20  Iteration 1165/3560 Training loss: 1.5441 6.8124 sec/batch\n",
      "Epoch 7/20  Iteration 1166/3560 Training loss: 1.5435 6.9820 sec/batch\n",
      "Epoch 7/20  Iteration 1167/3560 Training loss: 1.5430 6.9131 sec/batch\n",
      "Epoch 7/20  Iteration 1168/3560 Training loss: 1.5424 6.8831 sec/batch\n",
      "Epoch 7/20  Iteration 1169/3560 Training loss: 1.5423 6.7870 sec/batch\n",
      "Epoch 7/20  Iteration 1170/3560 Training loss: 1.5420 6.9433 sec/batch\n",
      "Epoch 7/20  Iteration 1171/3560 Training loss: 1.5416 6.9138 sec/batch\n",
      "Epoch 7/20  Iteration 1172/3560 Training loss: 1.5413 6.8231 sec/batch\n",
      "Epoch 7/20  Iteration 1173/3560 Training loss: 1.5409 6.8675 sec/batch\n",
      "Epoch 7/20  Iteration 1174/3560 Training loss: 1.5406 6.8674 sec/batch\n",
      "Epoch 7/20  Iteration 1175/3560 Training loss: 1.5404 6.8937 sec/batch\n",
      "Epoch 7/20  Iteration 1176/3560 Training loss: 1.5403 6.9091 sec/batch\n",
      "Epoch 7/20  Iteration 1177/3560 Training loss: 1.5400 6.9060 sec/batch\n",
      "Epoch 7/20  Iteration 1178/3560 Training loss: 1.5399 6.8149 sec/batch\n",
      "Epoch 7/20  Iteration 1179/3560 Training loss: 1.5394 6.8964 sec/batch\n",
      "Epoch 7/20  Iteration 1180/3560 Training loss: 1.5392 6.8614 sec/batch\n",
      "Epoch 7/20  Iteration 1181/3560 Training loss: 1.5388 6.8281 sec/batch\n",
      "Epoch 7/20  Iteration 1182/3560 Training loss: 1.5385 6.8641 sec/batch\n",
      "Epoch 7/20  Iteration 1183/3560 Training loss: 1.5380 6.9259 sec/batch\n",
      "Epoch 7/20  Iteration 1184/3560 Training loss: 1.5374 6.9171 sec/batch\n",
      "Epoch 7/20  Iteration 1185/3560 Training loss: 1.5372 6.8491 sec/batch\n",
      "Epoch 7/20  Iteration 1186/3560 Training loss: 1.5370 6.9038 sec/batch\n",
      "Epoch 7/20  Iteration 1187/3560 Training loss: 1.5367 6.8319 sec/batch\n",
      "Epoch 7/20  Iteration 1188/3560 Training loss: 1.5365 7.0925 sec/batch\n",
      "Epoch 7/20  Iteration 1189/3560 Training loss: 1.5362 6.9209 sec/batch\n",
      "Epoch 7/20  Iteration 1190/3560 Training loss: 1.5356 6.9158 sec/batch\n",
      "Epoch 7/20  Iteration 1191/3560 Training loss: 1.5351 6.7798 sec/batch\n",
      "Epoch 7/20  Iteration 1192/3560 Training loss: 1.5350 6.9344 sec/batch\n",
      "Epoch 7/20  Iteration 1193/3560 Training loss: 1.5347 6.9584 sec/batch\n",
      "Epoch 7/20  Iteration 1194/3560 Training loss: 1.5341 6.7958 sec/batch\n",
      "Epoch 7/20  Iteration 1195/3560 Training loss: 1.5340 6.9003 sec/batch\n",
      "Epoch 7/20  Iteration 1196/3560 Training loss: 1.5338 6.9210 sec/batch\n",
      "Epoch 7/20  Iteration 1197/3560 Training loss: 1.5334 6.9206 sec/batch\n",
      "Epoch 7/20  Iteration 1198/3560 Training loss: 1.5330 6.9159 sec/batch\n",
      "Epoch 7/20  Iteration 1199/3560 Training loss: 1.5325 6.8682 sec/batch\n",
      "Epoch 7/20  Iteration 1200/3560 Training loss: 1.5319 6.8092 sec/batch\n",
      "Validation loss: 1.38856 Saving checkpoint!\n",
      "Epoch 7/20  Iteration 1201/3560 Training loss: 1.5325 6.2006 sec/batch\n",
      "Epoch 7/20  Iteration 1202/3560 Training loss: 1.5323 6.4805 sec/batch\n",
      "Epoch 7/20  Iteration 1203/3560 Training loss: 1.5321 6.9963 sec/batch\n",
      "Epoch 7/20  Iteration 1204/3560 Training loss: 1.5320 6.7156 sec/batch\n",
      "Epoch 7/20  Iteration 1205/3560 Training loss: 1.5320 3602.5787 sec/batch\n",
      "Epoch 7/20  Iteration 1206/3560 Training loss: 1.5319 6.5220 sec/batch\n",
      "Epoch 7/20  Iteration 1207/3560 Training loss: 1.5318 7.1119 sec/batch\n",
      "Epoch 7/20  Iteration 1208/3560 Training loss: 1.5316 13.0293 sec/batch\n",
      "Epoch 7/20  Iteration 1209/3560 Training loss: 1.5318 11.2096 sec/batch\n",
      "Epoch 7/20  Iteration 1210/3560 Training loss: 1.5316 10.5333 sec/batch\n",
      "Epoch 7/20  Iteration 1211/3560 Training loss: 1.5314 11.1301 sec/batch\n",
      "Epoch 7/20  Iteration 1212/3560 Training loss: 1.5315 10.9399 sec/batch\n",
      "Epoch 7/20  Iteration 1213/3560 Training loss: 1.5312 11.0379 sec/batch\n",
      "Epoch 7/20  Iteration 1214/3560 Training loss: 1.5312 10.7779 sec/batch\n",
      "Epoch 7/20  Iteration 1215/3560 Training loss: 1.5310 10.8886 sec/batch\n",
      "Epoch 7/20  Iteration 1216/3560 Training loss: 1.5311 10.9730 sec/batch\n",
      "Epoch 7/20  Iteration 1217/3560 Training loss: 1.5310 11.2044 sec/batch\n",
      "Epoch 7/20  Iteration 1218/3560 Training loss: 1.5308 3604.0853 sec/batch\n",
      "Epoch 7/20  Iteration 1219/3560 Training loss: 1.5303 6.4437 sec/batch\n",
      "Epoch 7/20  Iteration 1220/3560 Training loss: 1.5301 12.5989 sec/batch\n",
      "Epoch 7/20  Iteration 1221/3560 Training loss: 1.5300 11.2516 sec/batch\n",
      "Epoch 7/20  Iteration 1222/3560 Training loss: 1.5299 10.6905 sec/batch\n",
      "Epoch 7/20  Iteration 1223/3560 Training loss: 1.5297 11.1026 sec/batch\n",
      "Epoch 7/20  Iteration 1224/3560 Training loss: 1.5295 10.8450 sec/batch\n",
      "Epoch 7/20  Iteration 1225/3560 Training loss: 1.5293 11.0239 sec/batch\n",
      "Epoch 7/20  Iteration 1226/3560 Training loss: 1.5291 10.8265 sec/batch\n",
      "Epoch 7/20  Iteration 1227/3560 Training loss: 1.5287 10.8077 sec/batch\n",
      "Epoch 7/20  Iteration 1228/3560 Training loss: 1.5287 10.9555 sec/batch\n",
      "Epoch 7/20  Iteration 1229/3560 Training loss: 1.5288 10.7663 sec/batch\n",
      "Epoch 7/20  Iteration 1230/3560 Training loss: 1.5286 2262.0078 sec/batch\n",
      "Epoch 7/20  Iteration 1231/3560 Training loss: 1.5285 4276.9914 sec/batch\n",
      "Epoch 7/20  Iteration 1232/3560 Training loss: 1.5284 6.6588 sec/batch\n",
      "Epoch 7/20  Iteration 1233/3560 Training loss: 1.5282 6.4698 sec/batch\n",
      "Epoch 7/20  Iteration 1234/3560 Training loss: 1.5280 6.4130 sec/batch\n",
      "Epoch 7/20  Iteration 1235/3560 Training loss: 1.5280 6.2555 sec/batch\n",
      "Epoch 7/20  Iteration 1236/3560 Training loss: 1.5283 6.5079 sec/batch\n",
      "Epoch 7/20  Iteration 1237/3560 Training loss: 1.5281 6.2247 sec/batch\n",
      "Epoch 7/20  Iteration 1238/3560 Training loss: 1.5279 6.2015 sec/batch\n",
      "Epoch 7/20  Iteration 1239/3560 Training loss: 1.5277 6.3502 sec/batch\n",
      "Epoch 7/20  Iteration 1240/3560 Training loss: 1.5273 6.3507 sec/batch\n",
      "Epoch 7/20  Iteration 1241/3560 Training loss: 1.5273 6.2662 sec/batch\n",
      "Epoch 7/20  Iteration 1242/3560 Training loss: 1.5271 6.3750 sec/batch\n",
      "Epoch 7/20  Iteration 1243/3560 Training loss: 1.5271 6.3009 sec/batch\n",
      "Epoch 7/20  Iteration 1244/3560 Training loss: 1.5268 6.3497 sec/batch\n",
      "Epoch 7/20  Iteration 1245/3560 Training loss: 1.5265 6.4157 sec/batch\n",
      "Epoch 7/20  Iteration 1246/3560 Training loss: 1.5264 6.3585 sec/batch\n",
      "Epoch 8/20  Iteration 1247/3560 Training loss: 1.5983 6.5571 sec/batch\n",
      "Epoch 8/20  Iteration 1248/3560 Training loss: 1.5478 6.5080 sec/batch\n",
      "Epoch 8/20  Iteration 1249/3560 Training loss: 1.5290 6.4076 sec/batch\n",
      "Epoch 8/20  Iteration 1250/3560 Training loss: 1.5218 6.5271 sec/batch\n",
      "Epoch 8/20  Iteration 1251/3560 Training loss: 1.5103 6.5299 sec/batch\n",
      "Epoch 8/20  Iteration 1252/3560 Training loss: 1.5004 6.9792 sec/batch\n",
      "Epoch 8/20  Iteration 1253/3560 Training loss: 1.5001 8.1430 sec/batch\n",
      "Epoch 8/20  Iteration 1254/3560 Training loss: 1.4961 8.7604 sec/batch\n",
      "Epoch 8/20  Iteration 1255/3560 Training loss: 1.4961 8.8722 sec/batch\n",
      "Epoch 8/20  Iteration 1256/3560 Training loss: 1.4955 8.3040 sec/batch\n",
      "Epoch 8/20  Iteration 1257/3560 Training loss: 1.4924 7.5830 sec/batch\n",
      "Epoch 8/20  Iteration 1258/3560 Training loss: 1.4919 7.0670 sec/batch\n",
      "Epoch 8/20  Iteration 1259/3560 Training loss: 1.4918 7.1766 sec/batch\n",
      "Epoch 8/20  Iteration 1260/3560 Training loss: 1.4948 6.5920 sec/batch\n",
      "Epoch 8/20  Iteration 1261/3560 Training loss: 1.4934 6.5539 sec/batch\n",
      "Epoch 8/20  Iteration 1262/3560 Training loss: 1.4914 6.5339 sec/batch\n",
      "Epoch 8/20  Iteration 1263/3560 Training loss: 1.4915 6.7950 sec/batch\n",
      "Epoch 8/20  Iteration 1264/3560 Training loss: 1.4928 6.6347 sec/batch\n",
      "Epoch 8/20  Iteration 1265/3560 Training loss: 1.4927 6.6921 sec/batch\n",
      "Epoch 8/20  Iteration 1266/3560 Training loss: 1.4937 6.6963 sec/batch\n",
      "Epoch 8/20  Iteration 1267/3560 Training loss: 1.4933 6.5915 sec/batch\n",
      "Epoch 8/20  Iteration 1268/3560 Training loss: 1.4932 6.7195 sec/batch\n",
      "Epoch 8/20  Iteration 1269/3560 Training loss: 1.4923 6.5772 sec/batch\n",
      "Epoch 8/20  Iteration 1270/3560 Training loss: 1.4918 6.6477 sec/batch\n",
      "Epoch 8/20  Iteration 1271/3560 Training loss: 1.4913 6.7147 sec/batch\n",
      "Epoch 8/20  Iteration 1272/3560 Training loss: 1.4893 6.9060 sec/batch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-780c3a10820d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                     model.initial_state: new_state}\n\u001b[1;32m     34\u001b[0m             batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n\u001b[0;32m---> 35\u001b[0;31m                                                  feed_dict=feed)\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Elendu/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Elendu/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Elendu/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/Elendu/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Elendu/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}_v{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saved checkpoints\n",
    "\n",
    "Read up on saving and loading checkpoints here: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/____.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
